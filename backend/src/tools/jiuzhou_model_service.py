# backend/src/tools/jiuzhou_model_service.py - ä¿®å¤ç‰ˆæœ¬

import os
import json
import logging
import asyncio
import aiohttp
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
from datetime import datetime, timedelta

# å¯¼å…¥é…ç½®
import sys

project_root = Path(__file__).parent.parent.parent.parent
sys.path.append(str(project_root))

from backend.config.config import settings, get_jiuzhou_config

logger = logging.getLogger(__name__)


class JiuzhouModelService:
    """ä¹å·åœ°ç†çŸ¥è¯†é—®ç­”æ¨¡å‹æœåŠ¡ - ä¿®å¤ç‰ˆ"""

    def __init__(self):
        # ä»é…ç½®ä¸­è¯»å–è®¾ç½®
        self.config = get_jiuzhou_config()
        self.base_url = self.config.base_url
        self.api_key = self.config.api_key
        self.publisher_name = self.config.publisher_name
        self.serving_name = self.config.serving_name
        self.model = self.config.model_name

        # ç¼“å­˜ç›¸å…³é…ç½®
        self.enable_cache = self.config.enable_analysis_cache
        self.cache_max_size = self.config.cache_max_size
        self.cache_ttl = self.config.cache_ttl
        self._cache = {}  # ç®€å•çš„å†…å­˜ç¼“å­˜
        self._cache_timestamps = {}  # ç¼“å­˜æ—¶é—´æˆ³

        # ğŸ”§ ä¿®å¤ï¼šç³»ç»Ÿæç¤ºè¯ä½œä¸ºå‰ç¼€ï¼Œè€Œä¸æ˜¯systemæ¶ˆæ¯
        self.system_prompt_prefix = "ä½œä¸ºä¸€ä½ä¸“ä¸šçš„åœ°ç†ä¿¡æ¯å’Œé¥æ„Ÿä¸“å®¶ï¼Œè¯·ç”¨ä¸­æ–‡å›ç­”ã€‚"

    def _get_cache_key(self, prompt: str, **kwargs) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        cache_data = {
            'prompt': prompt[:200],  # åªä½¿ç”¨å‰200ä¸ªå­—ç¬¦
            'temperature': kwargs.get('temperature', self.config.temperature),
            'model': self.model
        }
        return str(hash(json.dumps(cache_data, sort_keys=True)))

    def _get_from_cache(self, cache_key: str) -> Optional[str]:
        """ä»ç¼“å­˜è·å–ç»“æœ"""
        if not self.enable_cache:
            return None

        if cache_key in self._cache:
            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
            timestamp = self._cache_timestamps.get(cache_key, 0)
            if datetime.now().timestamp() - timestamp < self.cache_ttl:
                logger.debug(f"ç¼“å­˜å‘½ä¸­: {cache_key}")
                return self._cache[cache_key]
            else:
                # æ¸…ç†è¿‡æœŸç¼“å­˜
                del self._cache[cache_key]
                del self._cache_timestamps[cache_key]

        return None

    def _save_to_cache(self, cache_key: str, value: str):
        """ä¿å­˜åˆ°ç¼“å­˜"""
        if not self.enable_cache:
            return

        # æ£€æŸ¥ç¼“å­˜å¤§å°
        if len(self._cache) >= self.cache_max_size:
            # åˆ é™¤æœ€æ—§çš„ç¼“å­˜é¡¹
            oldest_key = min(self._cache_timestamps, key=self._cache_timestamps.get)
            del self._cache[oldest_key]
            del self._cache_timestamps[oldest_key]

        self._cache[cache_key] = value
        self._cache_timestamps[cache_key] = datetime.now().timestamp()
        logger.debug(f"ä¿å­˜åˆ°ç¼“å­˜: {cache_key}")

    async def analyze_user_requirements(self, user_input: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
        """ä½¿ç”¨ä¹å·æ¨¡å‹æ·±åº¦åˆ†æç”¨æˆ·éœ€æ±‚"""

        if not self.config.enable_ai_enhancement:
            logger.info("AIå¢å¼ºåŠŸèƒ½å·²ç¦ç”¨")
            return {"success": False, "error": "AI enhancement disabled"}

        prompt = self._build_requirement_analysis_prompt(user_input, context)

        # æ£€æŸ¥ç¼“å­˜
        cache_key = self._get_cache_key(prompt)
        cached_result = self._get_from_cache(cache_key)
        if cached_result:
            try:
                return json.loads(cached_result)
            except:
                pass

        try:
            response = await self._call_model(
                prompt,
                stream=False,  # åˆ†æä»»åŠ¡ä¸ä½¿ç”¨æµå¼
                temperature=0.7  # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨æ­£å¸¸æ¸©åº¦è€Œä¸æ˜¯0.35
            )
            result = self._parse_requirement_analysis(response)

            # ç¼“å­˜ç»“æœ
            if result.get("success", False):
                self._save_to_cache(cache_key, json.dumps(result))

            return result
        except Exception as e:
            logger.error(f"ä¹å·æ¨¡å‹åˆ†æéœ€æ±‚å¤±è´¥: {e}")
            return {"success": False, "error": str(e)}

    async def generate_intelligent_questions(
            self,
            user_input: str,
            missing_params: List[str],
            existing_params: Dict[str, Any],
            param_definitions: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæ™ºèƒ½åŒ–çš„æ¾„æ¸…é—®é¢˜"""

        if not self.config.enable_intelligent_questions:
            logger.info("æ™ºèƒ½é—®é¢˜ç”ŸæˆåŠŸèƒ½å·²ç¦ç”¨")
            return []

        prompt = self._build_intelligent_question_prompt(
            user_input, missing_params, existing_params, param_definitions
        )

        try:
            response = await self._call_model(
                prompt,
                stream=False,
                temperature=0.8  # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨åˆé€‚çš„æ¸©åº¦
            )
            return self._parse_clarification_questions(response)
        except Exception as e:
            logger.error(f"ä¹å·æ¨¡å‹ç”Ÿæˆé—®é¢˜å¤±è´¥: {e}")
            return []

    async def extract_implicit_parameters(self, text: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
        """æå–æ–‡æœ¬ä¸­çš„éšå«å‚æ•°"""

        if not self.config.enable_implicit_parameter_extraction:
            logger.info("éšå«å‚æ•°æå–åŠŸèƒ½å·²ç¦ç”¨")
            return {}

        # ğŸ”§ ä¿®å¤ï¼šå°†ç³»ç»Ÿæç¤ºæ•´åˆåˆ°ç”¨æˆ·æ¶ˆæ¯ä¸­
        prompt = f"""{self.system_prompt_prefix}æ·±åº¦åˆ†æä»¥ä¸‹æ–‡æœ¬ï¼Œæå–æ‰€æœ‰æ˜¾å¼å’Œéšå«çš„ç›‘æµ‹éœ€æ±‚å‚æ•°ã€‚

æ–‡æœ¬ï¼š{text}

è¯·åˆ†æå¹¶æå–ï¼š
1. æ˜¾å¼å‚æ•°ï¼ˆç›´æ¥æåˆ°çš„ï¼‰
2. éšå«å‚æ•°ï¼ˆå¯ä»¥æ¨æ–­çš„ï¼‰
3. å…³è”å‚æ•°ï¼ˆç›¸å…³ä½†æœªæåŠçš„ï¼‰

ä¾‹å¦‚ï¼š
- "ç›‘æµ‹æ°´è´¨" â†’ éšå«éœ€è¦å¤šå…‰è°±æ•°æ®ã€å®šæœŸè§‚æµ‹
- "åŸå¸‚æ‰©å¼ " â†’ éšå«éœ€è¦é«˜åˆ†è¾¨ç‡ã€å˜åŒ–æ£€æµ‹
- "åº”æ€¥å“åº”" â†’ éšå«éœ€è¦é«˜æ—¶æ•ˆæ€§ã€å…¨å¤©å€™è§‚æµ‹

è¿”å›JSONæ ¼å¼ï¼š
{{
    "explicit_params": {{
        "å‚æ•°å": "å‚æ•°å€¼"
    }},
    "implicit_params": {{
        "å‚æ•°å": {{
            "value": "æ¨æ–­å€¼",
            "reason": "æ¨æ–­ç†ç”±",
            "confidence": 0.9
        }}
    }},
    "suggested_params": {{
        "å‚æ•°å": {{
            "value": "å»ºè®®å€¼",
            "reason": "å»ºè®®ç†ç”±"
        }}
    }}
}}"""

        try:
            response = await self._call_model(
                prompt,
                stream=False,
                temperature=0.6  # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨é€‚ä¸­çš„æ¸©åº¦
            )
            return self._parse_json_response(response)
        except Exception as e:
            logger.error(f"æå–éšå«å‚æ•°å¤±è´¥: {e}")
            return {}

    async def _call_model(
            self,
            prompt: str,
            stream: bool = True,
            temperature: float = None,
            max_tokens: int = None
    ) -> str:
        """è°ƒç”¨ä¹å·æ¨¡å‹API - ä¿®å¤ç‰ˆ"""

        if temperature is None:
            temperature = self.config.temperature
        if max_tokens is None:
            max_tokens = self.config.max_tokens

        # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿æ¸©åº¦åœ¨åˆç†èŒƒå›´å†…
        temperature = max(0.1, min(1.0, temperature))

        # ğŸ”§ ä¿®å¤ï¼šä¸ä½¿ç”¨systemæ¶ˆæ¯ï¼Œå°†ç³»ç»Ÿæç¤ºæ•´åˆåˆ°useræ¶ˆæ¯ä¸­
        enhanced_prompt = f"{self.system_prompt_prefix}\n\n{prompt}"

        # æ„å»ºè¯·æ±‚ä½“
        payload = {
            "model": self.model,
            "messages": [
                {
                    "role": "user",
                    "content": enhanced_prompt
                }
            ],
            "stream": stream,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "top_p": self.config.top_p
        }

        # æ„å»ºè¯·æ±‚å¤´
        headers = {
            'Content-Type': 'application/json',
            'publisher-name': self.publisher_name,
            'api-key': self.api_key,
            'serving-name-en': self.serving_name,
        }

        logger.debug(f"ä¹å·APIè¯·æ±‚URL: {self.base_url}")
        logger.debug(f"ä¹å·APIè¯·æ±‚æ¸©åº¦: {temperature}")

        timeout = aiohttp.ClientTimeout(total=self.config.timeout)

        try:
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.post(
                        self.base_url,
                        headers=headers,
                        json=payload
                ) as response:
                    response_text = await response.text()

                    if response.status != 200:
                        logger.error(f"ä¹å·APIè¿”å›é”™è¯¯çŠ¶æ€ç : {response.status}")
                        logger.error(f"é”™è¯¯å“åº”å†…å®¹: {response_text}")
                        raise Exception(f"APIè¯·æ±‚å¤±è´¥: {response.status}, {response_text}")

                    if stream:
                        # ğŸ”§ ä¿®å¤ï¼šå¤„ç†æµå¼å“åº”ï¼Œæ­£ç¡®å¤„ç†Noneå€¼
                        full_content = ""
                        lines = response_text.strip().split('\n')

                        for line in lines:
                            line = line.strip()
                            if line.startswith('data: '):
                                try:
                                    # å¤„ç†ç‰¹æ®Šçš„ç»“æŸæ ‡è®°
                                    if line == 'data: [DONE]':
                                        break

                                    data = json.loads(line[6:])
                                    if 'choices' in data and data['choices']:
                                        delta = data['choices'][0].get('delta', {})
                                        # ğŸ”§ ä¿®å¤ï¼šæ£€æŸ¥contentæ˜¯å¦ä¸ºNone
                                        if 'content' in delta and delta['content'] is not None:
                                            full_content += delta['content']
                                except json.JSONDecodeError as e:
                                    logger.debug(f"è§£ææµå¼å“åº”è¡Œå¤±è´¥: {line}, é”™è¯¯: {e}")
                                    continue

                        logger.debug(f"æµå¼å“åº”å®Œæ•´å†…å®¹: {full_content[:200]}...")
                        return full_content
                    else:
                        # å¤„ç†éæµå¼å“åº”
                        try:
                            result = json.loads(response_text)
                            if 'choices' in result and result['choices']:
                                content = result['choices'][0]['message']['content']
                                logger.debug(f"éæµå¼å“åº”å†…å®¹: {content[:200]}...")
                                return content
                            else:
                                logger.error(f"APIè¿”å›æ ¼å¼é”™è¯¯: {response_text[:200]}")
                                raise Exception("APIè¿”å›æ ¼å¼é”™è¯¯")
                        except json.JSONDecodeError as e:
                            logger.error(f"è§£æéæµå¼å“åº”å¤±è´¥: {e}")
                            logger.error(f"åŸå§‹å“åº”: {response_text[:500]}")
                            raise Exception(f"APIå“åº”è§£æå¤±è´¥: {e}")

        except asyncio.TimeoutError:
            logger.error(f"ä¹å·APIè°ƒç”¨è¶…æ—¶ ({self.config.timeout}ç§’)")
            raise Exception(f"APIè°ƒç”¨è¶…æ—¶")
        except aiohttp.ClientError as e:
            logger.error(f"ç½‘ç»œè¯·æ±‚é”™è¯¯: {e}")
            raise Exception(f"ç½‘ç»œè¯·æ±‚é”™è¯¯: {e}")
        except Exception as e:
            logger.error(f"è°ƒç”¨ä¹å·APIæ—¶å‘ç”Ÿé”™è¯¯: {e}")
            raise

    def _build_requirement_analysis_prompt(self, user_input: str, context: Dict[str, Any]) -> str:
        """æ„å»ºéœ€æ±‚åˆ†ææç¤ºè¯"""

        context_str = ""
        if context:
            if context.get("conversation_history"):
                context_str += f"\nå¯¹è¯å†å²ï¼š{context['conversation_history']}"
            if context.get("user_profile"):
                context_str += f"\nç”¨æˆ·ç‰¹å¾ï¼š{context['user_profile']}"

        return f"""æ·±åº¦åˆ†æç”¨æˆ·çš„å«æ˜Ÿç›‘æµ‹éœ€æ±‚ï¼Œè¯†åˆ«æ‰€æœ‰å‚æ•°éœ€æ±‚ï¼ˆæ˜¾å¼å’Œéšå«ï¼‰ã€‚

ç”¨æˆ·è¾“å…¥ï¼š{user_input}{context_str}

è¯·åˆ†æï¼š
1. ç”¨æˆ·çš„æ ¸å¿ƒç›‘æµ‹æ„å›¾
2. å·²æ˜ç¡®æä¾›çš„å‚æ•°
3. å¯ä»¥æ¨æ–­çš„éšå«å‚æ•°
4. å¯èƒ½éœ€è¦ä½†æœªæåŠçš„å‚æ•°
5. å‚æ•°ä¹‹é—´çš„å…³è”æ€§

è¿”å›JSONæ ¼å¼ï¼š
{{
    "intent": {{
        "primary": "ä¸»è¦æ„å›¾",
        "secondary": ["æ¬¡è¦æ„å›¾1", "æ¬¡è¦æ„å›¾2"],
        "domain": "åº”ç”¨é¢†åŸŸ"
    }},
    "provided_params": {{
        "å‚æ•°å": {{
            "value": "å‚æ•°å€¼",
            "confidence": 0.9,
            "source": "explicit/implicit"
        }}
    }},
    "missing_params": {{
        "å‚æ•°å": {{
            "importance": "high/medium/low",
            "reason": "ä¸ºä»€ä¹ˆéœ€è¦",
            "default_applicable": true/false
        }}
    }},
    "param_relationships": [
        {{
            "params": ["å‚æ•°1", "å‚æ•°2"],
            "relationship": "å…³ç³»æè¿°"
        }}
    ],
    "recommendations": ["å»ºè®®1", "å»ºè®®2"]
}}"""

    def _build_intelligent_question_prompt(
            self,
            user_input: str,
            missing_params: List[str],
            existing_params: Dict[str, Any],
            param_definitions: Dict[str, Any]
    ) -> str:
        """æ„å»ºæ™ºèƒ½é—®é¢˜ç”Ÿæˆæç¤ºè¯"""

        param_info = {}
        for param in missing_params:
            for category in param_definitions.get("parameter_categories", {}).values():
                if param in category.get("parameters", {}):
                    param_info[param] = category["parameters"][param]

        return f"""åŸºäºç”¨æˆ·éœ€æ±‚å’Œä¸Šä¸‹æ–‡ï¼Œç”Ÿæˆè‡ªç„¶ã€æ™ºèƒ½çš„å‚æ•°æ¾„æ¸…é—®é¢˜ã€‚

ç”¨æˆ·éœ€æ±‚ï¼š{user_input}
å·²çŸ¥å‚æ•°ï¼š{json.dumps(existing_params, ensure_ascii=False)}
éœ€æ¾„æ¸…å‚æ•°ï¼š{missing_params}
å‚æ•°å®šä¹‰ï¼š{json.dumps(param_info, ensure_ascii=False)}

ç”Ÿæˆè¦æ±‚ï¼š
1. é—®é¢˜è¦è‡ªç„¶æµç•…ï¼Œåƒä¸“ä¸šé¡¾é—®çš„æé—®
2. æ ¹æ®ç”¨æˆ·éœ€æ±‚å®šåˆ¶é—®é¢˜å†…å®¹
3. æä¾›ç›¸å…³çš„é€‰é¡¹æˆ–ç¤ºä¾‹ï¼Œä½†è¦è´´åˆç”¨æˆ·åœºæ™¯
4. é—®é¢˜ä¹‹é—´è¦æœ‰é€»è¾‘å…³è”
5. ä¼˜å…ˆçº§è¦åˆç†ï¼ˆé‡è¦å‚æ•°å…ˆé—®ï¼‰
6. è¯­æ°”å‹å¥½ä¸“ä¸š

å¯¹æ¯ä¸ªå‚æ•°ç”Ÿæˆï¼š
{{
    "parameter_key": "å‚æ•°å",
    "question": "å®šåˆ¶åŒ–çš„æ¾„æ¸…é—®é¢˜",
    "type": "options/text/numeric",
    "options": [
        {{"value": "é€‰é¡¹å€¼", "label": "é€‰é¡¹æè¿°", "scenario": "é€‚ç”¨åœºæ™¯"}}
    ],
    "examples": ["è´´åˆç”¨æˆ·éœ€æ±‚çš„ç¤ºä¾‹"],
    "hint": "æ™ºèƒ½æç¤ºä¿¡æ¯",
    "priority": 1-10,
    "context_dependent": true/false,
    "follow_up": "å¯èƒ½çš„åç»­é—®é¢˜"
}}

è¿”å›JSONæ•°ç»„æ ¼å¼ã€‚"""

    def _parse_requirement_analysis(self, response: str) -> Dict[str, Any]:
        """è§£æéœ€æ±‚åˆ†æç»“æœ"""
        try:
            # æå–JSONéƒ¨åˆ†
            json_start = response.find('{')
            json_end = response.rfind('}') + 1
            if json_start >= 0 and json_end > json_start:
                json_str = response[json_start:json_end]
                result = json.loads(json_str)
                result["success"] = True
                return result
        except Exception as e:
            logger.error(f"è§£æéœ€æ±‚åˆ†æç»“æœå¤±è´¥: {e}, åŸå§‹å“åº”: {response[:200]}")

        return {
            "success": False,
            "intent": {"primary": "", "secondary": [], "domain": ""},
            "provided_params": {},
            "missing_params": {},
            "recommendations": []
        }

    def _parse_clarification_questions(self, response: str) -> List[Dict[str, Any]]:
        """è§£ææ¾„æ¸…é—®é¢˜"""
        try:
            # æå–JSONæ•°ç»„éƒ¨åˆ†
            json_start = response.find('[')
            json_end = response.rfind(']') + 1
            if json_start >= 0 and json_end > json_start:
                json_str = response[json_start:json_end]
                return json.loads(json_str)
        except Exception as e:
            logger.error(f"è§£ææ¾„æ¸…é—®é¢˜å¤±è´¥: {e}")

        return []

    def _parse_json_response(self, response: str) -> Dict[str, Any]:
        """é€šç”¨JSONå“åº”è§£æ"""
        try:
            # å°è¯•ç›´æ¥è§£æ
            return json.loads(response)
        except:
            try:
                # æå–JSONéƒ¨åˆ†
                json_start = response.find('{')
                json_end = response.rfind('}') + 1
                if json_start >= 0 and json_end > json_start:
                    json_str = response[json_start:json_end]
                    return json.loads(json_str)
            except Exception as e:
                logger.error(f"è§£æJSONå“åº”å¤±è´¥: {e}")

        return {}

    async def optimize_question_flow(
            self,
            questions: List[Dict[str, Any]],
            user_profile: Dict[str, Any] = None
    ) -> List[Dict[str, Any]]:
        """ä¼˜åŒ–é—®é¢˜æµç¨‹ï¼Œä½¿å…¶æ›´è‡ªç„¶"""

        prompt = f"""ä¼˜åŒ–ä»¥ä¸‹å‚æ•°æ¾„æ¸…é—®é¢˜ï¼Œä½¿å…¶æ›´è‡ªç„¶ã€æ›´ç¬¦åˆå¯¹è¯æµç¨‹ã€‚

åŸå§‹é—®é¢˜åˆ—è¡¨ï¼š
{json.dumps(questions, ensure_ascii=False, indent=2)}

ç”¨æˆ·ç”»åƒï¼š{json.dumps(user_profile or {}, ensure_ascii=False)}

ä¼˜åŒ–è¦æ±‚ï¼š
1. è°ƒæ•´é—®é¢˜é¡ºåºï¼Œä»æ˜“åˆ°éš¾ï¼Œä»æ¦‚æ‹¬åˆ°å…·ä½“
2. åˆå¹¶ç›¸å…³é—®é¢˜ï¼Œé¿å…é‡å¤
3. ä½¿é—®é¢˜æ›´å£è¯­åŒ–ã€æ›´å‹å¥½
4. æ ¹æ®å‚æ•°å…³è”æ€§åˆ†ç»„
5. æ·»åŠ å¼•å¯¼æ€§è¯´æ˜

è¿”å›ä¼˜åŒ–åçš„é—®é¢˜åˆ—è¡¨ï¼ˆJSONæ ¼å¼ï¼‰ã€‚"""

        try:
            response = await self._call_model(
                prompt,
                stream=False,
                temperature=0.7
            )
            return self._parse_clarification_questions(response)
        except Exception as e:
            logger.error(f"ä¼˜åŒ–é—®é¢˜æµç¨‹å¤±è´¥: {e}")
            return questions

    async def analyze_clarification_response(
            self,
            user_response: str,
            questions: List[Dict[str, Any]],
            context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """æ™ºèƒ½åˆ†æç”¨æˆ·çš„æ¾„æ¸…å›å¤"""

        prompt = f"""åˆ†æç”¨æˆ·å¯¹å‚æ•°æ¾„æ¸…é—®é¢˜çš„å›å¤ï¼Œæå–å‚æ•°å€¼å¹¶è¯†åˆ«æ–°çš„ä¿¡æ¯ã€‚

æ¾„æ¸…é—®é¢˜ï¼š
{json.dumps(questions, ensure_ascii=False, indent=2)}

ç”¨æˆ·å›å¤ï¼š{user_response}

å¯¹è¯ä¸Šä¸‹æ–‡ï¼š{json.dumps(context, ensure_ascii=False)}

è¯·åˆ†æï¼š
1. ç”¨æˆ·å›ç­”äº†å“ªäº›å‚æ•°
2. å›ç­”æ˜¯å¦æ˜ç¡®
3. æ˜¯å¦åŒ…å«æ–°çš„éœ€æ±‚ä¿¡æ¯
4. æ˜¯å¦éœ€è¦è¿›ä¸€æ­¥æ¾„æ¸…

è¿”å›JSONæ ¼å¼ï¼š
{{
    "answered_params": {{
        "å‚æ•°å": "æå–çš„å€¼"
    }},
    "unclear_params": {{
        "å‚æ•°å": "éœ€è¦æ¾„æ¸…çš„åŸå› "
    }},
    "new_requirements": ["æ–°éœ€æ±‚1", "æ–°éœ€æ±‚2"],
    "follow_up_needed": true/false,
    "confidence": 0.9
}}"""

        try:
            response = await self._call_model(
                prompt,
                stream=False,
                temperature=0.6
            )
            return self._parse_json_response(response)
        except Exception as e:
            logger.error(f"åˆ†ææ¾„æ¸…å›å¤å¤±è´¥: {e}")
            return {}

    async def generate_parameter_recommendations(
            self,
            partial_params: Dict[str, Any],
            user_intent: str
    ) -> Dict[str, Any]:
        """åŸºäºéƒ¨åˆ†å‚æ•°ç”Ÿæˆæ™ºèƒ½æ¨è"""

        if not self.config.enable_parameter_recommendations:
            logger.info("å‚æ•°æ¨èåŠŸèƒ½å·²ç¦ç”¨")
            return {}

        prompt = f"""åŸºäºç”¨æˆ·æ„å›¾å’Œå·²æœ‰å‚æ•°ï¼Œæ¨èå…¶ä»–å‚æ•°çš„æœ€ä¼˜å€¼ã€‚

ç”¨æˆ·æ„å›¾ï¼š{user_intent}
å·²æœ‰å‚æ•°ï¼š{json.dumps(partial_params, ensure_ascii=False, indent=2)}

è¯·æ¨èï¼š
1. å…¶ä»–å‚æ•°çš„æœ€ä¼˜å€¼
2. å‚æ•°ç»„åˆçš„ååŒæ•ˆåº”
3. å¯èƒ½çš„ä¼˜åŒ–å»ºè®®

è€ƒè™‘å› ç´ ï¼š
- æŠ€æœ¯å¯è¡Œæ€§
- æˆæœ¬æ•ˆç›Š
- æ•°æ®è´¨é‡
- å®é™…åº”ç”¨æ•ˆæœ

è¿”å›JSONæ ¼å¼çš„æ¨èã€‚"""

        try:
            response = await self._call_model(
                prompt,
                stream=False,
                temperature=0.7
            )
            return self._parse_json_response(response)
        except Exception as e:
            logger.error(f"ç”Ÿæˆå‚æ•°æ¨èå¤±è´¥: {e}")
            return {}

    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        self._cache.clear()
        self._cache_timestamps.clear()
        logger.info("å·²æ¸…ç©ºä¹å·æ¨¡å‹ç¼“å­˜")


# åˆ›å»ºå•ä¾‹å®ä¾‹
jiuzhou_service = JiuzhouModelService()


# ä¾¿æ·å‡½æ•°
async def analyze_requirements_with_jiuzhou(user_input: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
    """ä½¿ç”¨ä¹å·æ¨¡å‹åˆ†æç”¨æˆ·éœ€æ±‚"""
    return await jiuzhou_service.analyze_user_requirements(user_input, context)


async def generate_smart_questions(
        user_input: str,
        missing_params: List[str],
        existing_params: Dict[str, Any],
        param_definitions: Dict[str, Any]
) -> List[Dict[str, Any]]:
    """ä½¿ç”¨ä¹å·æ¨¡å‹ç”Ÿæˆæ™ºèƒ½æ¾„æ¸…é—®é¢˜"""
    return await jiuzhou_service.generate_intelligent_questions(
        user_input, missing_params, existing_params, param_definitions
    )


async def extract_implicit_params(text: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
    """æå–éšå«å‚æ•°"""
    return await jiuzhou_service.extract_implicit_parameters(text, context)


async def analyze_user_clarification(
        response: str,
        questions: List[Dict[str, Any]],
        context: Dict[str, Any]
) -> Dict[str, Any]:
    """åˆ†æç”¨æˆ·çš„æ¾„æ¸…å›å¤"""
    return await jiuzhou_service.analyze_clarification_response(response, questions, context)


async def get_param_recommendations(
        partial_params: Dict[str, Any],
        user_intent: str
) -> Dict[str, Any]:
    """è·å–å‚æ•°æ¨è"""
    return await jiuzhou_service.generate_parameter_recommendations(partial_params, user_intent)


# æµ‹è¯•ä¿®å¤
if __name__ == "__main__":
    async def test_fixed_service():
        print("æµ‹è¯•ä¿®å¤åçš„ä¹å·æ¨¡å‹æœåŠ¡...")

        # æµ‹è¯•1ï¼šåŸºç¡€è°ƒç”¨
        print("\n1. æµ‹è¯•åŸºç¡€è°ƒç”¨ï¼ˆæ— systemæ¶ˆæ¯ï¼‰")
        result = await jiuzhou_service._call_model(
            "ä»€ä¹ˆæ˜¯é¥æ„Ÿå«æ˜Ÿï¼Ÿ",
            stream=False,
            temperature=0.7
        )
        print(f"å“åº”: {result[:100]}...")

        # æµ‹è¯•2ï¼šæµå¼è°ƒç”¨
        print("\n2. æµ‹è¯•æµå¼è°ƒç”¨")
        result = await jiuzhou_service._call_model(
            "è¯·ä»‹ç»ä¸€ä¸‹å«æ˜Ÿç›‘æµ‹æ°´è´¨çš„æ–¹æ³•",
            stream=True,
            temperature=0.7
        )
        print(f"å“åº”: {result[:100]}...")

        # æµ‹è¯•3ï¼šéœ€æ±‚åˆ†æ
        print("\n3. æµ‹è¯•éœ€æ±‚åˆ†æåŠŸèƒ½")
        analysis = await analyze_requirements_with_jiuzhou(
            "æˆ‘éœ€è¦ç›‘æµ‹é’æµ·æ¹–çš„æ°´è´¨å˜åŒ–"
        )
        print(f"åˆ†æç»“æœ: {json.dumps(analysis, ensure_ascii=False, indent=2)}")


    asyncio.run(test_fixed_service())