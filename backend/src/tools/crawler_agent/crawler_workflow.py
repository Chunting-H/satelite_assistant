# backend/src/tools/crawler_agent/crawler_workflow.py

import os
import sys
import json
import logging
import asyncio
import uuid
from datetime import datetime
from typing import Dict, Any, List, Optional, Callable
from pathlib import Path

# ç¡®ä¿é¡¹ç›®æ ¹ç›®å½•åœ¨sys.pathä¸­
current_file = Path(__file__).resolve()
project_root = current_file.parent.parent.parent.parent
sys.path.append(str(project_root))

from backend.config.config import settings
from .state import CrawlerState, CrawlJob
from .nodes import crawler_nodes

logger = logging.getLogger(__name__)


class CrawlerWorkflow:
    """æ™ºèƒ½çˆ¬è™«å·¥ä½œæµç®¡ç†å™¨"""
    
    def __init__(self):
        self.active_jobs: Dict[str, CrawlJob] = {}
        self.job_history: List[CrawlJob] = []
        self.nodes = crawler_nodes
        
        # å®šä¹‰å·¥ä½œæµè·¯å¾„
        self.workflow_graph = {
            "START": "parameter_parsing",
            "parameter_parsing": "web_crawler", 
            "web_crawler": "data_cleaning",
            "data_cleaning": "duplicate_check",
            "duplicate_check": "file_write",
            "file_write": "logging",
            "logging": "END",
            "error_handler": "END"
        }
    
    async def create_crawl_job(
        self, 
        target_sites: List[str] = None, 
        keywords: List[str] = None,
        max_satellites: int = 10
    ) -> str:
        """åˆ›å»ºçˆ¬å–ä»»åŠ¡"""
        try:
            job_id = str(uuid.uuid4())
            
            # åˆ›å»ºä»»åŠ¡å¯¹è±¡
            job = CrawlJob(
                job_id=job_id,
                target_sites=target_sites or ["Gunter's Space Page"],
                keywords=keywords or [],
                max_satellites=max_satellites,
                status="pending"
            )
            
            self.active_jobs[job_id] = job
            
            logger.info(f"ğŸ“‹ åˆ›å»ºçˆ¬å–ä»»åŠ¡: {job_id}")
            return job_id
            
        except Exception as e:
            logger.error(f"åˆ›å»ºçˆ¬å–ä»»åŠ¡å¤±è´¥: {str(e)}")
            raise
    
    async def execute_crawl_job(
        self, 
        job_id: str,
        progress_callback: Optional[Callable] = None
    ) -> Dict[str, Any]:
        """æ‰§è¡Œçˆ¬å–ä»»åŠ¡"""
        try:
            if job_id not in self.active_jobs:
                raise ValueError(f"ä»»åŠ¡ä¸å­˜åœ¨: {job_id}")
            
            job = self.active_jobs[job_id]
            job.status = "running"
            job.started_at = datetime.now().timestamp()
            
            logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œçˆ¬å–ä»»åŠ¡: {job_id}")
            
            # åˆ›å»ºåˆå§‹çŠ¶æ€
            state: CrawlerState = {
                "task_id": job_id,
                "target_sites": job.target_sites,
                "keywords": job.keywords,
                "max_satellites": job.max_satellites,
                "raw_satellite_data": [],
                "formatted_satellite_data": [],
                "new_satellites_count": 0,
                "existing_satellites_count": 0,
                "failed_satellites_count": 0,
                "processing_errors": [],
                "crawl_start_time": None,
                "crawl_end_time": None,
                "execution_time": 0.0,
                "log_file_path": "",
                "current_node": "START",
                "error_occurred": False,
                "error_message": "",
                "storage_stats": {}
            }
            
            # æ‰§è¡Œå·¥ä½œæµ
            final_state = await self._execute_workflow(state, progress_callback)
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            job.completed_at = datetime.now().timestamp()
            if final_state['error_occurred']:
                job.status = "failed"
                job.error_message = final_state['error_message']
            else:
                job.status = "completed"
            
            # ä¿å­˜ç»“æœ
            job.results = {
                "new_satellites": final_state['new_satellites_count'],
                "existing_satellites": final_state['existing_satellites_count'],
                "failed_satellites": final_state['failed_satellites_count'],
                "execution_time": final_state['execution_time'],
                "log_file_path": final_state['log_file_path'],
                "storage_stats": final_state['storage_stats']
            }
            
            # ç§»åŠ¨åˆ°å†å²è®°å½•
            self.job_history.append(job)
            del self.active_jobs[job_id]
            
            logger.info(f"âœ… çˆ¬å–ä»»åŠ¡å®Œæˆ: {job_id}")
            
            return {
                "job_id": job_id,
                "status": job.status,
                "results": job.results,
                "error_message": job.error_message
            }
            
        except Exception as e:
            logger.error(f"æ‰§è¡Œçˆ¬å–ä»»åŠ¡å¤±è´¥: {job_id}, é”™è¯¯: {str(e)}")
            
            # æ›´æ–°ä»»åŠ¡ä¸ºå¤±è´¥çŠ¶æ€
            if job_id in self.active_jobs:
                job = self.active_jobs[job_id]
                job.status = "failed"
                job.error_message = str(e)
                job.completed_at = datetime.now().timestamp()
                
                self.job_history.append(job)
                del self.active_jobs[job_id]
            
            raise
    
    async def _execute_workflow(
        self, 
        state: CrawlerState, 
        progress_callback: Optional[Callable] = None
    ) -> CrawlerState:
        """æ‰§è¡Œå·¥ä½œæµå›¾"""
        try:
            current_node = "parameter_parsing"
            
            while current_node != "END":
                logger.info(f"ğŸ“ æ‰§è¡ŒèŠ‚ç‚¹: {current_node}")
                
                # å‘é€è¿›åº¦æ›´æ–°
                if progress_callback:
                    await progress_callback({
                        "current_node": current_node,
                        "task_id": state["task_id"]
                    })
                
                # æ‰§è¡Œå½“å‰èŠ‚ç‚¹
                if current_node == "parameter_parsing":
                    state = await self.nodes.parameter_parsing_node(state)
                elif current_node == "web_crawler":
                    state = await self.nodes.web_crawler_node(state)
                elif current_node == "data_cleaning":
                    state = await self.nodes.data_cleaning_node(state)
                elif current_node == "duplicate_check":
                    state = await self.nodes.duplicate_check_node(state)
                elif current_node == "file_write":
                    state = await self.nodes.file_write_node(state)
                elif current_node == "logging":
                    state = await self.nodes.logging_node(state)
                elif current_node == "error_handler":
                    state = await self.nodes.error_handler_node(state)
                else:
                    raise ValueError(f"æœªçŸ¥èŠ‚ç‚¹: {current_node}")
                
                # æ£€æŸ¥æ˜¯å¦å‡ºé”™
                if state['error_occurred']:
                    logger.error(f"èŠ‚ç‚¹æ‰§è¡Œå‡ºé”™: {current_node}, é”™è¯¯: {state['error_message']}")
                    current_node = "error_handler"
                else:
                    # è·å–ä¸‹ä¸€ä¸ªèŠ‚ç‚¹
                    current_node = self.workflow_graph.get(current_node, "END")
                
                # æ·»åŠ å°å»¶è¿Ÿï¼Œé¿å…è¿‡å¿«æ‰§è¡Œ
                await asyncio.sleep(0.1)
            
            logger.info(f"ğŸ å·¥ä½œæµæ‰§è¡Œå®Œæˆ: {state['task_id']}")
            return state
            
        except Exception as e:
            logger.error(f"å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {str(e)}")
            state['error_occurred'] = True
            state['error_message'] = f"å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {str(e)}"
            
            # æ‰§è¡Œé”™è¯¯å¤„ç†èŠ‚ç‚¹
            state = await self.nodes.error_handler_node(state)
            return state
    
    def get_job_status(self, job_id: str) -> Optional[Dict[str, Any]]:
        """è·å–ä»»åŠ¡çŠ¶æ€"""
        # æ£€æŸ¥æ´»è·ƒä»»åŠ¡
        if job_id in self.active_jobs:
            job = self.active_jobs[job_id]
            return job.to_dict()
        
        # æ£€æŸ¥å†å²ä»»åŠ¡
        for job in self.job_history:
            if job.job_id == job_id:
                return job.to_dict()
        
        return None
    
    def list_jobs(self, status: Optional[str] = None) -> List[Dict[str, Any]]:
        """åˆ—å‡ºä»»åŠ¡"""
        all_jobs = list(self.active_jobs.values()) + self.job_history
        
        if status:
            all_jobs = [job for job in all_jobs if job.status == status]
        
        # æŒ‰åˆ›å»ºæ—¶é—´å€’åºæ’åˆ—
        all_jobs.sort(key=lambda x: x.created_at, reverse=True)
        
        return [job.to_dict() for job in all_jobs]
    
    async def get_crawl_logs(self, limit: int = 50) -> List[Dict[str, Any]]:
        """è·å–çˆ¬å–æ—¥å¿—"""
        try:
            logs_file = os.path.join(settings.data_dir, "crawlLogs.json")
            
            if not os.path.exists(logs_file):
                return []
            
            with open(logs_file, 'r', encoding='utf-8') as f:
                logs = json.load(f)
                
                if not isinstance(logs, list):
                    return []
                
                # æŒ‰æ—¶é—´å€’åºæ’åˆ—ï¼Œè¿”å›æœ€è¿‘çš„è®°å½•
                logs.sort(key=lambda x: x.get('crawlTime', ''), reverse=True)
                return logs[:limit]
                
        except Exception as e:
            logger.error(f"è·å–çˆ¬å–æ—¥å¿—å¤±è´¥: {str(e)}")
            return []
    
    async def get_crawl_statistics(self, days: int = 30) -> Dict[str, Any]:
        """è·å–çˆ¬å–ç»Ÿè®¡ä¿¡æ¯"""
        try:
            logs = await self.get_crawl_logs(limit=1000)
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            total_crawls = len(logs)
            total_new_satellites = sum(log.get('newDataCount', 0) for log in logs)
            total_failed = sum(log.get('failedCount', 0) for log in logs)
            
            # æŒ‰æ—¥æœŸåˆ†ç»„ç»Ÿè®¡
            daily_stats = {}
            for log in logs:
                date_str = log.get('crawlTime', '')[:10]  # è·å–æ—¥æœŸéƒ¨åˆ†
                if date_str not in daily_stats:
                    daily_stats[date_str] = {
                        'date': date_str,
                        'crawls': 0,
                        'new_satellites': 0,
                        'failed': 0
                    }
                
                daily_stats[date_str]['crawls'] += 1
                daily_stats[date_str]['new_satellites'] += log.get('newDataCount', 0)
                daily_stats[date_str]['failed'] += log.get('failedCount', 0)
            
            # æŒ‰ç«™ç‚¹ç»Ÿè®¡
            site_stats = {}
            for log in logs:
                for site in log.get('targetSites', []):
                    if site not in site_stats:
                        site_stats[site] = {
                            'site': site,
                            'crawls': 0,
                            'new_satellites': 0
                        }
                    
                    site_stats[site]['crawls'] += 1
                    site_stats[site]['new_satellites'] += log.get('newDataCount', 0)
            
            return {
                'total_crawls': total_crawls,
                'total_new_satellites': total_new_satellites,
                'total_failed': total_failed,
                'daily_stats': list(daily_stats.values()),
                'site_stats': list(site_stats.values()),
                'recent_logs': logs[:10]  # æœ€è¿‘10æ¡æ—¥å¿—
            }
            
        except Exception as e:
            logger.error(f"è·å–çˆ¬å–ç»Ÿè®¡å¤±è´¥: {str(e)}")
            return {
                'total_crawls': 0,
                'total_new_satellites': 0,
                'total_failed': 0,
                'daily_stats': [],
                'site_stats': [],
                'recent_logs': []
            }


# åˆ›å»ºå…¨å±€å·¥ä½œæµå®ä¾‹
crawler_workflow = CrawlerWorkflow()
