# backend/src/tools/crawler_agent/nodes.py

import os
import sys
import json
import logging
import asyncio
from datetime import datetime
from typing import Dict, Any, List, Optional
from pathlib import Path

# ç¡®ä¿é¡¹ç›®æ ¹ç›®å½•åœ¨sys.pathä¸­
current_file = Path(__file__).resolve()
project_root = current_file.parent.parent.parent.parent
sys.path.append(str(project_root))

from backend.config.config import settings
from backend.src.llm.multi_model_manager import get_multi_model_manager
from backend.src.tools.sate_search.satellite_crawler import SatelliteCrawler
from backend.src.tools.sate_search.satellite_data_processor import SatelliteDataProcessor
from .state import CrawlerState, CrawlLogEntry

logger = logging.getLogger(__name__)


class CrawlerNodes:
    """çˆ¬è™«å·¥ä½œæµèŠ‚ç‚¹é›†åˆ"""
    
    def __init__(self):
        self.crawler = SatelliteCrawler()
        self.processor = SatelliteDataProcessor()
        self.model_manager = get_multi_model_manager()
        
    async def parameter_parsing_node(self, state: CrawlerState) -> CrawlerState:
        """å‚æ•°è§£æèŠ‚ç‚¹ - è§£æå’ŒéªŒè¯ä»»åŠ¡å‚æ•°"""
        try:
            logger.info(f"ğŸ”§ å‚æ•°è§£æèŠ‚ç‚¹: ä»»åŠ¡ID {state['task_id']}")
            
            # éªŒè¯ç›®æ ‡ç«™ç‚¹
            supported_sites = ["Gunter's Space Page", "NASA EO Portal"]
            if not state['target_sites']:
                state['target_sites'] = ["Gunter's Space Page"]  # é»˜è®¤ç«™ç‚¹
            
            # éªŒè¯å…³é”®è¯ï¼ˆå¯é€‰ï¼‰
            if not state['keywords']:
                state['keywords'] = []
            
            # éªŒè¯æœ€å¤§å«æ˜Ÿæ•°é‡
            if state['max_satellites'] <= 0:
                state['max_satellites'] = 10
            elif state['max_satellites'] > 50:
                state['max_satellites'] = 50  # é™åˆ¶æœ€å¤§æ•°é‡
            
            # è®¾ç½®èµ·å§‹æ—¶é—´
            state['crawl_start_time'] = datetime.now().timestamp()
            state['current_node'] = 'parameter_parsing'
            state['error_occurred'] = False
            
            logger.info(f"âœ… å‚æ•°è§£æå®Œæˆ: ç«™ç‚¹{state['target_sites']}, å…³é”®è¯{state['keywords']}, æœ€å¤§æ•°é‡{state['max_satellites']}")
            
            return state
            
        except Exception as e:
            logger.error(f"âŒ å‚æ•°è§£æèŠ‚ç‚¹å‡ºé”™: {str(e)}")
            state['error_occurred'] = True
            state['error_message'] = f"å‚æ•°è§£æå¤±è´¥: {str(e)}"
            return state
    
    async def web_crawler_node(self, state: CrawlerState) -> CrawlerState:
        """ç½‘é¡µçˆ¬è™«èŠ‚ç‚¹ - çˆ¬å–å«æ˜Ÿæ•°æ®"""
        try:
            logger.info(f"ğŸ•·ï¸ ç½‘é¡µçˆ¬è™«èŠ‚ç‚¹: å¼€å§‹çˆ¬å– {state['target_sites']}")
            
            state['current_node'] = 'web_crawler'
            raw_data = []
            
            for site in state['target_sites']:
                if site == "Gunter's Space Page":
                    # çˆ¬å–Gunter's Space Page
                    site_data = await self.crawler.crawl_recent_satellites(
                        max_satellites=state['max_satellites']
                    )
                    raw_data.extend(site_data)
                    logger.info(f"ğŸ“¡ ä» {site} çˆ¬å–åˆ° {len(site_data)} ä¸ªå«æ˜Ÿ")
                
                # è¿™é‡Œå¯ä»¥æ·»åŠ å…¶ä»–ç½‘ç«™çš„çˆ¬å–é€»è¾‘
                # elif site == "NASA EO Portal":
                #     pass
            
            state['raw_satellite_data'] = raw_data
            
            if not raw_data:
                logger.warning("âš ï¸ æœªçˆ¬å–åˆ°ä»»ä½•å«æ˜Ÿæ•°æ®")
                state['processing_errors'] = ["æœªçˆ¬å–åˆ°ä»»ä½•å«æ˜Ÿæ•°æ®"]
            else:
                logger.info(f"âœ… çˆ¬è™«èŠ‚ç‚¹å®Œæˆ: æ€»å…±çˆ¬å– {len(raw_data)} ä¸ªå«æ˜Ÿ")
            
            return state
            
        except Exception as e:
            logger.error(f"âŒ ç½‘é¡µçˆ¬è™«èŠ‚ç‚¹å‡ºé”™: {str(e)}")
            state['error_occurred'] = True
            state['error_message'] = f"ç½‘é¡µçˆ¬è™«å¤±è´¥: {str(e)}"
            return state
    
    async def data_cleaning_node(self, state: CrawlerState) -> CrawlerState:
        """æ•°æ®æ¸…æ´—èŠ‚ç‚¹ - ä½¿ç”¨å¤§æ¨¡å‹ç»“æ„åŒ–æ•°æ®"""
        try:
            logger.info(f"ğŸ§¹ æ•°æ®æ¸…æ´—èŠ‚ç‚¹: å¤„ç† {len(state['raw_satellite_data'])} ä¸ªåŸå§‹æ•°æ®")
            
            state['current_node'] = 'data_cleaning'
            
            if not state['raw_satellite_data']:
                logger.warning("âš ï¸ æ²¡æœ‰åŸå§‹æ•°æ®éœ€è¦æ¸…æ´—")
                state['formatted_satellite_data'] = []
                return state
            
            # ä½¿ç”¨æ•°æ®å¤„ç†å™¨è¿›è¡Œç»“æ„åŒ–
            formatted_data = await self.processor.clean_and_format_data(
                state['raw_satellite_data']
            )
            
            state['formatted_satellite_data'] = formatted_data
            
            # ç»Ÿè®¡å¤„ç†ç»“æœ
            success_count = len([d for d in formatted_data if d.get('satelliteName')])
            failed_count = len(state['raw_satellite_data']) - success_count
            
            logger.info(f"âœ… æ•°æ®æ¸…æ´—å®Œæˆ: æˆåŠŸ{success_count}ä¸ª, å¤±è´¥{failed_count}ä¸ª")
            
            return state
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®æ¸…æ´—èŠ‚ç‚¹å‡ºé”™: {str(e)}")
            state['error_occurred'] = True
            state['error_message'] = f"æ•°æ®æ¸…æ´—å¤±è´¥: {str(e)}"
            return state
    
    async def duplicate_check_node(self, state: CrawlerState) -> CrawlerState:
        """é‡å¤æ£€æŸ¥èŠ‚ç‚¹ - æ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦å·²å­˜åœ¨"""
        try:
            logger.info(f"ğŸ” é‡å¤æ£€æŸ¥èŠ‚ç‚¹: æ£€æŸ¥ {len(state['formatted_satellite_data'])} ä¸ªæ ¼å¼åŒ–æ•°æ®")
            
            state['current_node'] = 'duplicate_check'
            
            if not state['formatted_satellite_data']:
                logger.warning("âš ï¸ æ²¡æœ‰æ ¼å¼åŒ–æ•°æ®éœ€è¦æ£€æŸ¥")
                state['new_satellites_count'] = 0
                state['existing_satellites_count'] = 0
                return state
            
            # ä½¿ç”¨æ•°æ®å¤„ç†å™¨æ£€æŸ¥é‡å¤å’Œå­˜å‚¨
            storage_stats = await self.processor.check_and_store_satellites(
                state['formatted_satellite_data']
            )
            
            state['storage_stats'] = storage_stats
            state['new_satellites_count'] = storage_stats.get('new_satellites', 0)
            state['existing_satellites_count'] = storage_stats.get('existing_satellites', 0)
            state['failed_satellites_count'] = storage_stats.get('errors', 0)
            
            logger.info(f"âœ… é‡å¤æ£€æŸ¥å®Œæˆ: æ–°å¢{state['new_satellites_count']}ä¸ª, å·²å­˜åœ¨{state['existing_satellites_count']}ä¸ª")
            
            return state
            
        except Exception as e:
            logger.error(f"âŒ é‡å¤æ£€æŸ¥èŠ‚ç‚¹å‡ºé”™: {str(e)}")
            state['error_occurred'] = True
            state['error_message'] = f"é‡å¤æ£€æŸ¥å¤±è´¥: {str(e)}"
            return state
    
    async def file_write_node(self, state: CrawlerState) -> CrawlerState:
        """æ–‡ä»¶å†™å…¥èŠ‚ç‚¹ - å·²åœ¨duplicate_check_nodeä¸­å¤„ç†"""
        try:
            logger.info(f"ğŸ’¾ æ–‡ä»¶å†™å…¥èŠ‚ç‚¹: æ•°æ®å·²åœ¨é‡å¤æ£€æŸ¥èŠ‚ç‚¹ä¸­å†™å…¥")
            
            state['current_node'] = 'file_write'
            
            # æ•°æ®å·²åœ¨duplicate_check_nodeä¸­å†™å…¥åˆ°eo_satellite.json
            # è¿™é‡Œä¸»è¦æ˜¯ç¡®è®¤æ“ä½œå®Œæˆ
            
            if state['new_satellites_count'] > 0:
                logger.info(f"âœ… æ–‡ä»¶å†™å…¥å®Œæˆ: æ–°å¢ {state['new_satellites_count']} ä¸ªå«æ˜Ÿåˆ°æ•°æ®åº“")
            else:
                logger.info("â„¹ï¸ æ²¡æœ‰æ–°å«æ˜Ÿéœ€è¦å†™å…¥æ•°æ®åº“")
            
            return state
            
        except Exception as e:
            logger.error(f"âŒ æ–‡ä»¶å†™å…¥èŠ‚ç‚¹å‡ºé”™: {str(e)}")
            state['error_occurred'] = True
            state['error_message'] = f"æ–‡ä»¶å†™å…¥å¤±è´¥: {str(e)}"
            return state
    
    async def logging_node(self, state: CrawlerState) -> CrawlerState:
        """æ—¥å¿—è®°å½•èŠ‚ç‚¹ - è®°å½•çˆ¬å–ç»“æœ"""
        try:
            logger.info(f"ğŸ“ æ—¥å¿—è®°å½•èŠ‚ç‚¹: åˆ›å»ºçˆ¬å–æ—¥å¿—")
            
            state['current_node'] = 'logging'
            state['crawl_end_time'] = datetime.now().timestamp()
            state['execution_time'] = state['crawl_end_time'] - state['crawl_start_time']
            
            # åˆ›å»ºæ—¥å¿—æ¡ç›®
            log_entry = CrawlLogEntry()
            log_entry.target_sites = state['target_sites']
            log_entry.new_data_count = state['new_satellites_count']
            log_entry.updated_data_count = 0  # æš‚æ—¶ä¸æ”¯æŒæ›´æ–°
            log_entry.failed_count = state['failed_satellites_count']
            log_entry.fail_reasons = state['processing_errors']
            log_entry.execution_time = state['execution_time']
            log_entry.total_processed = len(state['raw_satellite_data'])
            
            # æå–å«æ˜Ÿåç§°åˆ—è¡¨
            log_entry.data_list = [
                sat.get('satelliteName', 'Unknown') 
                for sat in state['formatted_satellite_data']
                if sat.get('satelliteName')
            ]
            
            # ä¿å­˜åˆ°crawlLogs.json
            log_file_path = await self._save_crawl_log(log_entry)
            state['log_file_path'] = log_file_path
            
            # åŒæ—¶åˆ›å»ºè¯¦ç»†çš„æ‰§è¡Œæ—¥å¿—
            detailed_log_path = self.processor.create_crawl_log(
                state['storage_stats'], 
                state['execution_time'], 
                ', '.join(state['target_sites'])
            )
            
            logger.info(f"âœ… æ—¥å¿—è®°å½•å®Œæˆ: {log_file_path}")
            
            return state
            
        except Exception as e:
            logger.error(f"âŒ æ—¥å¿—è®°å½•èŠ‚ç‚¹å‡ºé”™: {str(e)}")
            state['error_occurred'] = True
            state['error_message'] = f"æ—¥å¿—è®°å½•å¤±è´¥: {str(e)}"
            return state
    
    async def _save_crawl_log(self, log_entry: CrawlLogEntry) -> str:
        """ä¿å­˜çˆ¬å–æ—¥å¿—åˆ°crawlLogs.json"""
        try:
            logs_file = os.path.join(settings.data_dir, "crawlLogs.json")
            
            # è¯»å–ç°æœ‰æ—¥å¿—
            existing_logs = []
            if os.path.exists(logs_file):
                try:
                    with open(logs_file, 'r', encoding='utf-8') as f:
                        existing_logs = json.load(f)
                        if not isinstance(existing_logs, list):
                            existing_logs = []
                except Exception as e:
                    logger.warning(f"è¯»å–ç°æœ‰æ—¥å¿—å¤±è´¥: {e}")
                    existing_logs = []
            
            # æ·»åŠ æ–°æ—¥å¿—æ¡ç›®
            existing_logs.append(log_entry.to_dict())
            
            # ä¿æŒæœ€è¿‘100æ¡æ—¥å¿—
            if len(existing_logs) > 100:
                existing_logs = existing_logs[-100:]
            
            # ä¿å­˜æ—¥å¿—æ–‡ä»¶
            os.makedirs(os.path.dirname(logs_file), exist_ok=True)
            with open(logs_file, 'w', encoding='utf-8') as f:
                json.dump(existing_logs, f, ensure_ascii=False, indent=2)
            
            return logs_file
            
        except Exception as e:
            logger.error(f"ä¿å­˜çˆ¬å–æ—¥å¿—å¤±è´¥: {e}")
            return ""
    
    async def error_handler_node(self, state: CrawlerState) -> CrawlerState:
        """é”™è¯¯å¤„ç†èŠ‚ç‚¹"""
        try:
            logger.error(f"âŒ é”™è¯¯å¤„ç†èŠ‚ç‚¹: {state['error_message']}")
            
            state['current_node'] = 'error_handler'
            state['crawl_end_time'] = datetime.now().timestamp()
            state['execution_time'] = state['crawl_end_time'] - state['crawl_start_time']
            
            # åˆ›å»ºé”™è¯¯æ—¥å¿—
            log_entry = CrawlLogEntry()
            log_entry.target_sites = state['target_sites']
            log_entry.failed_count = 1
            log_entry.fail_reasons = [state['error_message']]
            log_entry.execution_time = state['execution_time']
            
            # ä¿å­˜é”™è¯¯æ—¥å¿—
            log_file_path = await self._save_crawl_log(log_entry)
            state['log_file_path'] = log_file_path
            
            return state
            
        except Exception as e:
            logger.error(f"é”™è¯¯å¤„ç†èŠ‚ç‚¹ä¹Ÿå‡ºé”™äº†: {str(e)}")
            return state


# åˆ›å»ºå…¨å±€èŠ‚ç‚¹å®ä¾‹
crawler_nodes = CrawlerNodes()
