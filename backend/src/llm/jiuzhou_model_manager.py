# backend/src/llm/jiuzhou_model_manager.py

import os
import torch
import json
import logging
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM
import asyncio
from concurrent.futures import ThreadPoolExecutor
import time
logger = logging.getLogger(__name__)


class JiuzhouModelManager:
    """ä¹å·æ¨¡å‹ç®¡ç†å™¨ - å¤„ç†æ¨¡å‹åŠ è½½å’Œæ¨ç†"""

    def __init__(self, model_path: str = None):
        self.model_path = model_path or "/root/autodl-tmp/virtual_constellation_assistant/backend/src/llm/JiuZhou-Instruct-v0.2"
        self.device = torch.device("cuda:0") if torch.cuda.is_available() else torch.device("cpu")
        self.model = None
        self.tokenizer = None
        self.executor = ThreadPoolExecutor(max_workers=1)
        self._initialized = False

        # åŠ è½½ç¤ºä¾‹æ¡ˆä¾‹
        self.example_cases = self._load_example_cases()

    def _load_example_cases(self) -> List[Dict]:
        """åŠ è½½è™šæ‹Ÿæ˜Ÿåº§å°æ ·æœ¬æ¡ˆä¾‹"""
        examples_path = Path(__file__).parent.parent.parent.parent / "backend/data" / "example_constellations.json"
        try:
            with open(examples_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return data.get("example_plans", [])
        except Exception as e:
            logger.error(f"åŠ è½½ç¤ºä¾‹æ¡ˆä¾‹å¤±è´¥: {e}")
            return []

    def initialize(self):
        """å»¶è¿Ÿåˆå§‹åŒ–æ¨¡å‹"""
        if self._initialized:
            logger.info("ä¹å·æ¨¡å‹å·²ç»åˆå§‹åŒ–ï¼Œè·³è¿‡é‡å¤åˆå§‹åŒ–")
            return

        max_retries = 3
        retry_count = 0

        while retry_count < max_retries:
            try:
                logger.info(f"æ­£åœ¨åŠ è½½ä¹å·æ¨¡å‹ (å°è¯• {retry_count + 1}/{max_retries}): {self.model_path}")
                start_time = time.time()

                # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
                if not os.path.exists(self.model_path):
                    raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.model_path}")

                self.tokenizer = AutoTokenizer.from_pretrained(
                    self.model_path,
                    trust_remote_code=True
                )

                # è®¾ç½®pad_token
                if self.tokenizer.pad_token is None:
                    self.tokenizer.pad_token = self.tokenizer.eos_token

                # æ ¹æ®å¯ç”¨è®¾å¤‡åŠ è½½æ¨¡å‹
                if torch.cuda.is_available():
                    logger.info(f"ä½¿ç”¨GPUåŠ è½½æ¨¡å‹: {self.device}")
                    self.model = AutoModelForCausalLM.from_pretrained(
                        self.model_path,
                        torch_dtype=torch.bfloat16,
                        device_map="cuda:0",
                        trust_remote_code=True,
                        low_cpu_mem_usage=True  # å‡å°‘CPUå†…å­˜ä½¿ç”¨
                    )
                else:
                    logger.info("ä½¿ç”¨CPUåŠ è½½æ¨¡å‹")
                    self.model = AutoModelForCausalLM.from_pretrained(
                        self.model_path,
                        torch_dtype=torch.float32,
                        device_map="cpu",
                        trust_remote_code=True,
                        low_cpu_mem_usage=True
                    )

                self._initialized = True
                load_time = time.time() - start_time
                logger.info(f"âœ… ä¹å·æ¨¡å‹åŠ è½½æˆåŠŸï¼Œè€—æ—¶: {load_time:.2f}ç§’")

                # é¢„çƒ­æ¨¡å‹
                self._warmup_model()

                return  # æˆåŠŸåŠ è½½ï¼Œé€€å‡ºé‡è¯•å¾ªç¯

            except Exception as e:
                retry_count += 1
                logger.error(f"ä¹å·æ¨¡å‹åŠ è½½å¤±è´¥ (å°è¯• {retry_count}/{max_retries}): {e}")

                if retry_count < max_retries:
                    wait_time = retry_count * 2  # é€’å¢ç­‰å¾…æ—¶é—´
                    logger.info(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                else:
                    logger.error("ä¹å·æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
                    raise

    def _warmup_model(self):
        """é¢„çƒ­æ¨¡å‹ï¼Œè¿›è¡Œä¸€æ¬¡ç®€å•çš„æ¨ç†"""
        try:
            logger.info("å¼€å§‹é¢„çƒ­ä¹å·æ¨¡å‹...")
            warmup_prompt = "ä½ å¥½"
            self._sync_generate(warmup_prompt, max_tokens=10)
            logger.info("ä¹å·æ¨¡å‹é¢„çƒ­å®Œæˆ")
        except Exception as e:
            logger.warning(f"æ¨¡å‹é¢„çƒ­å¤±è´¥: {e}")




    def _sync_generate(self, prompt: str, max_tokens: int = 600) -> str:
        """åŒæ­¥ç”Ÿæˆæ–‡æœ¬"""
        if not self._initialized:
            self.initialize()

        try:
            messages = [{"role": "user", "content": prompt}]

            # ä¿®å¤ï¼šæ­£ç¡®å¤„ç†tokenizerè¾“å‡º
            # å…ˆæ£€æŸ¥tokenizeræ˜¯å¦æœ‰apply_chat_templateæ–¹æ³•
            if hasattr(self.tokenizer, 'apply_chat_template'):
                # ä½¿ç”¨apply_chat_template
                input_text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
                inputs = self.tokenizer(input_text, return_tensors="pt", padding=True, truncation=True)
            else:
                # å¤‡ç”¨æ–¹æ³•ï¼šç›´æ¥ä½¿ç”¨prompt
                inputs = self.tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)

            # å°†inputsç§»åˆ°æ­£ç¡®çš„è®¾å¤‡
            input_ids = inputs.input_ids.to(self.device)
            attention_mask = inputs.attention_mask.to(self.device) if 'attention_mask' in inputs else None

            with torch.no_grad():
                # æ„å»ºç”Ÿæˆå‚æ•°
                generate_kwargs = {
                    "input_ids": input_ids,
                    "max_new_tokens": max_tokens,
                    "do_sample": True,
                    "temperature": 0.7,
                    "top_p": 0.9,
                    "pad_token_id": self.tokenizer.pad_token_id,
                    "eos_token_id": self.tokenizer.eos_token_id,
                }

                # å¦‚æœæœ‰attention_maskï¼Œæ·»åŠ åˆ°å‚æ•°ä¸­
                if attention_mask is not None:
                    generate_kwargs["attention_mask"] = attention_mask

                outputs_id = self.model.generate(**generate_kwargs)

            # è§£ç è¾“å‡º
            outputs = self.tokenizer.batch_decode(outputs_id, skip_special_tokens=True)[0]

            # æå–ç”Ÿæˆçš„éƒ¨åˆ†ï¼ˆå»é™¤è¾“å…¥éƒ¨åˆ†ï¼‰
            if len(outputs) > len(prompt):
                response = outputs[len(prompt):].strip()
            else:
                response = outputs

            # å°è¯•æå–åŠ©æ‰‹å›å¤
            if "assistant" in response:
                return response.split("assistant")[-1].strip()

            return response

        except Exception as e:
            logger.error(f"ç”Ÿæˆæ–‡æœ¬æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            # è¿”å›ä¸€ä¸ªé»˜è®¤å“åº”è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
            return "æŠ±æ­‰ï¼Œç”Ÿæˆå›å¤æ—¶å‡ºç°é”™è¯¯ã€‚"

    async def identify_missing_parameters(
            self,
            user_context: str,
            existing_params: Dict[str, Any],
            conversation_history: str = "",
            domain_knowledge: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """ä½¿ç”¨ä¹å·æ¨¡å‹æ™ºèƒ½è¯†åˆ«ç¼ºå¤±çš„å‚æ•°

        Args:
            user_context: ç”¨æˆ·çš„éœ€æ±‚æè¿°
            existing_params: å·²ç»è¯†åˆ«å‡ºçš„å‚æ•°
            conversation_history: å¯¹è¯å†å²
            domain_knowledge: é¢†åŸŸçŸ¥è¯†ï¼ˆå¯é€‰ï¼‰

        Returns:
            åŒ…å«ç¼ºå¤±å‚æ•°åˆ—è¡¨å’Œåˆ†æè¯´æ˜çš„å­—å…¸
        """
        try:
            # æ„å»ºæç¤ºè¯
            prompt = self._build_identify_missing_params_prompt(
                user_context,
                existing_params,
                conversation_history,
                domain_knowledge
            )

            # è°ƒç”¨æ¨¡å‹
            response = await self.generate(prompt, max_tokens=1000)

            # è§£æå“åº”
            result = self._parse_missing_params_response(response)

            return result

        except Exception as e:
            logger.error(f"è¯†åˆ«ç¼ºå¤±å‚æ•°æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return {
                "missing_parameters": [],
                "analysis_notes": f"AIåˆ†æå¤±è´¥: {str(e)}"
            }

    def _build_identify_missing_params_prompt(
            self,
            user_context: str,
            existing_params: Dict[str, Any],
            conversation_history: str,
            domain_knowledge: Dict[str, Any] = None
    ) -> str:
        """æ„å»ºè¯†åˆ«ç¼ºå¤±å‚æ•°çš„æç¤ºè¯"""

        # é¢†åŸŸçŸ¥è¯†ç¤ºä¾‹
        domain_examples = """
    ## è™šæ‹Ÿæ˜Ÿåº§è®¾è®¡çš„å…¸å‹å‚æ•°éœ€æ±‚ï¼š

    ### 1. æ°´è´¨ç›‘æµ‹åœºæ™¯
    å¿…éœ€å‚æ•°ï¼šç›‘æµ‹ç›®æ ‡ï¼ˆæ°´è´¨ï¼‰ã€è§‚æµ‹åŒºåŸŸã€è§‚æµ‹é¢‘ç‡ï¼ˆå»ºè®®æ¯å‘¨2æ¬¡ä»¥ä¸Šï¼‰ã€å…‰è°±æ³¢æ®µï¼ˆå¤šå…‰è°±æˆ–é«˜å…‰è°±ï¼‰
    é‡è¦å‚æ•°ï¼šç›‘æµ‹å‘¨æœŸã€åˆ†æéœ€æ±‚ï¼ˆæ°´è´¨å‚æ•°åæ¼”ï¼‰ã€ç©ºé—´åˆ†è¾¨ç‡ï¼ˆ10-30ç±³ï¼‰
    å¯é€‰å‚æ•°ï¼šæ—¶æ•ˆæ€§è¦æ±‚ã€è¾“å‡ºæ ¼å¼

    ### 2. å†œä¸šç›‘æµ‹åœºæ™¯  
    å¿…éœ€å‚æ•°ï¼šç›‘æµ‹ç›®æ ‡ï¼ˆå†œä¸š/ä½œç‰©ï¼‰ã€è§‚æµ‹åŒºåŸŸã€ç›‘æµ‹å‘¨æœŸï¼ˆè¦†ç›–ç”Ÿé•¿å­£ï¼‰ã€ç©ºé—´åˆ†è¾¨ç‡
    é‡è¦å‚æ•°ï¼šå…‰è°±æ³¢æ®µï¼ˆå«çº¢è¾¹æ³¢æ®µï¼‰ã€è§‚æµ‹é¢‘ç‡ï¼ˆå…³é”®ç”Ÿè‚²æœŸåŠ å¯†ï¼‰ã€åˆ†æéœ€æ±‚
    å¯é€‰å‚æ•°ï¼šç²¾åº¦è¦æ±‚ã€å¤©æ°”ä¾èµ–æ€§

    ### 3. åŸå¸‚ç›‘æµ‹åœºæ™¯
    å¿…éœ€å‚æ•°ï¼šç›‘æµ‹ç›®æ ‡ï¼ˆåŸå¸‚æ‰©å¼ /å»ºç­‘ï¼‰ã€è§‚æµ‹åŒºåŸŸã€ç©ºé—´åˆ†è¾¨ç‡ï¼ˆé«˜åˆ†è¾¨ç‡<5ç±³ï¼‰
    é‡è¦å‚æ•°ï¼šè§‚æµ‹é¢‘ç‡ã€åˆ†æéœ€æ±‚ï¼ˆå˜åŒ–æ£€æµ‹ï¼‰ã€ç›‘æµ‹å‘¨æœŸ
    å¯é€‰å‚æ•°ï¼šæ•°æ®å¤„ç†çº§åˆ«ã€è¾“å‡ºæ ¼å¼

    ### 4. ç¾å®³åº”æ€¥åœºæ™¯
    å¿…éœ€å‚æ•°ï¼šç›‘æµ‹ç›®æ ‡ï¼ˆå…·ä½“ç¾å®³ç±»å‹ï¼‰ã€è§‚æµ‹åŒºåŸŸã€æ—¶æ•ˆæ€§è¦æ±‚ï¼ˆå‡†å®æ—¶ï¼‰
    é‡è¦å‚æ•°ï¼šè§‚æµ‹é¢‘ç‡ï¼ˆé«˜é¢‘ï¼‰ã€å¤©æ°”ä¾èµ–æ€§ï¼ˆå…¨å¤©å€™ï¼‰ã€å“åº”æ—¶é—´
    å¯é€‰å‚æ•°ï¼šæ•°æ®å®‰å…¨è¦æ±‚ã€è¾“å‡ºæ ¼å¼
    """

        prompt = f"""ä½ æ˜¯ä¸€ä¸ªç»éªŒä¸°å¯Œçš„è™šæ‹Ÿæ˜Ÿåº§è®¾è®¡ä¸“å®¶ï¼Œç²¾é€šé¥æ„Ÿåº”ç”¨å’Œå«æ˜Ÿä»»åŠ¡è§„åˆ’ã€‚
    è¯·åˆ†æç”¨æˆ·çš„éœ€æ±‚ï¼Œè¯†åˆ«å‡ºè®¾è®¡è™šæ‹Ÿæ˜Ÿåº§æ–¹æ¡ˆæ‰€éœ€ä½†å°šæœªæä¾›çš„å…³é”®å‚æ•°ã€‚

    {domain_examples}

    ## å‚æ•°é‡è¦æ€§åˆ¤æ–­åŸåˆ™ï¼š
    1. **é«˜é‡è¦æ€§(high)**ï¼šç¼ºå°‘è¯¥å‚æ•°å°†æ— æ³•è®¾è®¡æœ‰æ•ˆæ–¹æ¡ˆ
    2. **ä¸­é‡è¦æ€§(medium)**ï¼šè¯¥å‚æ•°ä¼šæ˜¾è‘—å½±å“æ–¹æ¡ˆè´¨é‡
    3. **ä½é‡è¦æ€§(low)**ï¼šè¯¥å‚æ•°æ˜¯é”¦ä¸Šæ·»èŠ±ï¼Œå¯ä»¥ä½¿ç”¨é»˜è®¤å€¼

    ## ç”¨æˆ·éœ€æ±‚ï¼š
    {user_context}

    ## å·²è¯†åˆ«å‚æ•°ï¼š
    {self._format_existing_params(existing_params)}

    ## å¯¹è¯å†å²ï¼š
    {conversation_history if conversation_history else "ï¼ˆæ— å†å²å¯¹è¯ï¼‰"}

    ## åˆ†æä»»åŠ¡ï¼š
    1. ç†è§£ç”¨æˆ·çš„æ ¸å¿ƒéœ€æ±‚å’Œåº”ç”¨åœºæ™¯
    2. åŸºäºä¸“ä¸šçŸ¥è¯†åˆ¤æ–­å“ªäº›å‚æ•°æ˜¯å¿…éœ€çš„
    3. è€ƒè™‘å‚æ•°ä¹‹é—´çš„å…³è”æ€§å’Œä¾èµ–å…³ç³»
    4. é¿å…è¿‡åº¦è¯¢é—®ï¼Œåªè¯†åˆ«çœŸæ­£é‡è¦çš„ç¼ºå¤±å‚æ•°
    5. å¦‚æœç”¨æˆ·éœ€æ±‚å·²ç»è¶³å¤Ÿæ˜ç¡®ï¼Œå¯ä»¥åªè¯†åˆ«1-2ä¸ªæœ€å…³é”®çš„å‚æ•°

    ## è¾“å‡ºè¦æ±‚ï¼š
    è¯·ä»¥JSONæ ¼å¼è¾“å‡ºåˆ†æç»“æœï¼ŒåŒ…å«ï¼š
    - missing_parameters: ç¼ºå¤±å‚æ•°åˆ—è¡¨
    - analysis_notes: æ•´ä½“åˆ†æè¯´æ˜
    - scenario_type: è¯†åˆ«çš„åœºæ™¯ç±»å‹

    ç¤ºä¾‹è¾“å‡ºï¼š
    {{
      "scenario_type": "æ°´è´¨ç›‘æµ‹",
      "missing_parameters": [
        {{
          "parameter": "observation_frequency",
          "name": "è§‚æµ‹é¢‘ç‡",
          "importance": "high",
          "reason": "æ°´è´¨ç›‘æµ‹éœ€è¦è¶³å¤Ÿçš„æ—¶é—´åˆ†è¾¨ç‡æ¥æ•æ‰æ°´ä½“å˜åŒ–ï¼Œå»ºè®®æ¯å‘¨2æ¬¡ä»¥ä¸Š",
          "suggested_default": "æ¯å‘¨2æ¬¡",
          "related_params": ["monitoring_period", "time_criticality"]
        }}
      ],
      "analysis_notes": "ç”¨æˆ·éœ€è¦ç›‘æµ‹é’æµ·æ¹–æ°´è´¨ï¼Œå·²æœ‰åŒºåŸŸå’Œç›®æ ‡ï¼Œä½†ç¼ºå°‘æ—¶é—´ç»´åº¦çš„å‚æ•°ã€‚æ°´è´¨å˜åŒ–å…·æœ‰æ—¶é—´åŠ¨æ€æ€§ï¼Œéœ€è¦æ˜ç¡®è§‚æµ‹é¢‘ç‡ã€‚"
    }}
    """

        return prompt

    def _format_existing_params(self, params: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–å·²æœ‰å‚æ•°ç”¨äºæç¤ºè¯"""
        if not params:
            return "ï¼ˆæš‚æ— å·²è¯†åˆ«å‚æ•°ï¼‰"

        formatted = []
        param_names = {
            "monitoring_target": "ç›‘æµ‹ç›®æ ‡",
            "observation_area": "è§‚æµ‹åŒºåŸŸ",
            "observation_frequency": "è§‚æµ‹é¢‘ç‡",
            "spatial_resolution": "ç©ºé—´åˆ†è¾¨ç‡",
            "monitoring_period": "ç›‘æµ‹å‘¨æœŸ",
            "spectral_bands": "å…‰è°±æ³¢æ®µ",
            "analysis_requirements": "åˆ†æéœ€æ±‚",
            "time_criticality": "æ—¶æ•ˆæ€§è¦æ±‚"
        }

        for key, value in params.items():
            name = param_names.get(key, key)
            formatted.append(f"- {name}: {value}")

        return "\n".join(formatted)

    def _parse_missing_params_response(self, model_output: str) -> Dict[str, Any]:
        """è§£ææ¨¡å‹è¯†åˆ«çš„ç¼ºå¤±å‚æ•°"""
        try:
            import re
            import json

            # æå–JSONéƒ¨åˆ†
            json_match = re.search(r'\{[\s\S]*\}', model_output)
            if json_match:
                result = json.loads(json_match.group())

                # éªŒè¯è¾“å‡ºæ ¼å¼
                if 'missing_parameters' in result:
                    # ç¡®ä¿æ¯ä¸ªå‚æ•°éƒ½æœ‰å¿…è¦çš„å­—æ®µ
                    for param in result.get('missing_parameters', []):
                        if 'parameter' not in param:
                            continue
                        # è®¾ç½®é»˜è®¤å€¼
                        param.setdefault('importance', 'medium')
                        param.setdefault('reason', 'éœ€è¦è¯¥å‚æ•°ä»¥å®Œå–„æ–¹æ¡ˆè®¾è®¡')
                        param.setdefault('name', param['parameter'])

                    return result
                else:
                    logger.warning("æ¨¡å‹è¾“å‡ºç¼ºå°‘missing_parameterså­—æ®µ")
                    return {
                        "missing_parameters": [],
                        "analysis_notes": "AIåˆ†æç»“æœæ ¼å¼ä¸æ­£ç¡®"
                    }

        except Exception as e:
            logger.error(f"è§£æç¼ºå¤±å‚æ•°å“åº”å¤±è´¥: {e}")
            logger.debug(f"åŸå§‹è¾“å‡º: {model_output[:500]}...")

        return {
            "missing_parameters": [],
            "analysis_notes": "è§£æAIå“åº”å¤±è´¥"
        }

    async def generate_contextual_questions(
            self,
            missing_params_info: List[Dict[str, Any]],
            user_profile: Dict[str, Any] = None
    ) -> List[Dict[str, Any]]:
        """åŸºäºç¼ºå¤±å‚æ•°ä¿¡æ¯ç”Ÿæˆä¸Šä¸‹æ–‡ç›¸å…³çš„é—®é¢˜

        è¿™ä¸ªæ–¹æ³•ä¼šæ ¹æ®ç”¨æˆ·ç”»åƒå’Œåœºæ™¯ç”Ÿæˆæ›´åŠ ä¸ªæ€§åŒ–çš„é—®é¢˜
        """
        try:
            prompt = f"""ä½œä¸ºè™šæ‹Ÿæ˜Ÿåº§è®¾è®¡åŠ©æ‰‹ï¼Œè¯·ä¸ºä»¥ä¸‹ç¼ºå¤±å‚æ•°ç”Ÿæˆå‹å¥½ã€ä¸“ä¸šçš„æ¾„æ¸…é—®é¢˜ã€‚

    ## ç¼ºå¤±å‚æ•°ä¿¡æ¯ï¼š
    {json.dumps(missing_params_info, ensure_ascii=False, indent=2)}

    ## ç”¨æˆ·ç”»åƒï¼š
    {json.dumps(user_profile, ensure_ascii=False) if user_profile else "æ™®é€šç”¨æˆ·"}

    ## ç”Ÿæˆè¦æ±‚ï¼š
    1. é—®é¢˜è¦è‡ªç„¶ã€å‹å¥½ã€æ˜“æ‡‚
    2. æ ¹æ®å‚æ•°çš„é‡è¦æ€§è°ƒæ•´é—®é¢˜çš„è¯¦ç»†ç¨‹åº¦
    3. é«˜é‡è¦æ€§å‚æ•°è¦è§£é‡Šä¸ºä»€ä¹ˆéœ€è¦è¿™ä¸ªä¿¡æ¯
    4. æä¾›å…·ä½“çš„ä¾‹å­å¸®åŠ©ç”¨æˆ·ç†è§£
    5. è€ƒè™‘å‚æ•°ä¹‹é—´çš„å…³è”ï¼Œå¯ä»¥åœ¨ä¸€ä¸ªé—®é¢˜ä¸­è¯¢é—®ç›¸å…³çš„å¤šä¸ªå‚æ•°

    ## è¾“å‡ºæ ¼å¼ï¼š
    {{
      "questions": [
        {{
          "parameter_key": "å‚æ•°é”®å",
          "question": "é—®é¢˜æ–‡æœ¬",
          "explanation": "ä¸ºä»€ä¹ˆéœ€è¦è¿™ä¸ªä¿¡æ¯ï¼ˆå¯é€‰ï¼‰",
          "examples": ["ç¤ºä¾‹1", "ç¤ºä¾‹2"],
          "quick_options": ["å¿«é€Ÿé€‰é¡¹1", "å¿«é€Ÿé€‰é¡¹2"],
          "allow_custom": true
        }}
      ]
    }}
    """

            response = await self.generate(prompt, max_tokens=800)
            return self._parse_contextual_questions(response)

        except Exception as e:
            logger.error(f"ç”Ÿæˆä¸Šä¸‹æ–‡é—®é¢˜å¤±è´¥: {e}")
            # è¿”å›åŸºç¡€é—®é¢˜
            return self._generate_basic_questions(missing_params_info)

    def _parse_contextual_questions(self, model_output: str) -> List[Dict[str, Any]]:
        """è§£æç”Ÿæˆçš„ä¸Šä¸‹æ–‡é—®é¢˜"""
        try:
            import re
            import json

            json_match = re.search(r'\{[\s\S]*\}', model_output)
            if json_match:
                result = json.loads(json_match.group())
                return result.get('questions', [])
        except Exception as e:
            logger.error(f"è§£æä¸Šä¸‹æ–‡é—®é¢˜å¤±è´¥: {e}")

        return []

    def _generate_basic_questions(self, missing_params_info: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ç”ŸæˆåŸºç¡€é—®é¢˜ä½œä¸ºå¤‡é€‰"""
        questions = []

        for param_info in missing_params_info:
            param_key = param_info.get('parameter')
            param_name = param_info.get('name', param_key)
            reason = param_info.get('reason', '')

            question = {
                'parameter_key': param_key,
                'question': f'è¯·æä¾›{param_name}ä¿¡æ¯',
                'explanation': reason,
                'examples': [],
                'quick_options': [],
                'allow_custom': True
            }

            # æ ¹æ®å‚æ•°ç±»å‹å®šåˆ¶é—®é¢˜
            if param_key == 'observation_frequency':
                question['question'] = 'æ‚¨éœ€è¦å¤šé•¿æ—¶é—´è·å–ä¸€æ¬¡è§‚æµ‹æ•°æ®ï¼Ÿ'
                question['examples'] = ['æ¯å¤©1æ¬¡', 'æ¯å‘¨2æ¬¡', 'æ¯æœˆ1æ¬¡']
                question['quick_options'] = ['æ¯å¤©1æ¬¡', 'æ¯å‘¨2æ¬¡', 'æ¯æœˆ1æ¬¡', 'å®æ—¶ç›‘æµ‹']
            elif param_key == 'spatial_resolution':
                question['question'] = 'æ‚¨éœ€è¦ä»€ä¹ˆçº§åˆ«çš„å›¾åƒæ¸…æ™°åº¦ï¼Ÿ'
                question['examples'] = ['é«˜æ¸…æ™°åº¦(èƒ½çœ‹æ¸…å»ºç­‘ç‰©)', 'ä¸­ç­‰æ¸…æ™°åº¦(èƒ½çœ‹æ¸…è¡—é“)', 'ä¸€èˆ¬æ¸…æ™°åº¦(èƒ½çœ‹æ¸…åŒºåŸŸ)']
                question['quick_options'] = ['é«˜(<5ç±³)', 'ä¸­(5-30ç±³)', 'ä½(>30ç±³)']

            questions.append(question)

        return questions

    async def generate(self, prompt: str, max_tokens: int = 600) -> str:
        """å¼‚æ­¥ç”Ÿæˆæ–‡æœ¬"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(self.executor, self._sync_generate, prompt, max_tokens)

    async def extract_parameters(self, user_input: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
        """ä½¿ç”¨ä¹å·æ¨¡å‹æ™ºèƒ½æå–å‚æ•°"""
        try:
            # æ·»åŠ è¾“å…¥éªŒè¯
            if not user_input or not isinstance(user_input, str):
                logger.warning(f"æ— æ•ˆçš„ç”¨æˆ·è¾“å…¥: {type(user_input)}")
                return {}

            logger.info(f"å¼€å§‹æå–å‚æ•°ï¼Œç”¨æˆ·è¾“å…¥: {user_input[:100]}...")

            # æ„å»ºæç¤º
            prompt = self._build_parameter_extraction_prompt(user_input, context)
            logger.debug(f"æ„å»ºçš„æç¤ºè¯é•¿åº¦: {len(prompt)}")

            # è°ƒç”¨æ¨¡å‹
            response = await self.generate(prompt, max_tokens=800)
            logger.info(f"æ¨¡å‹å“åº”é•¿åº¦: {len(response)}")

            # è§£æè¾“å‡º
            extracted_params = self._parse_parameter_extraction(response)
            logger.info(f"æˆåŠŸæå–å‚æ•°: {extracted_params}")

            return extracted_params

        except Exception as e:
            logger.error(f"å‚æ•°æå–è¿‡ç¨‹å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return {}

    def _build_parameter_extraction_prompt(self, user_input: str, context: Dict[str, Any] = None) -> str:
        """æ„å»ºå‚æ•°æå–çš„æç¤ºè¯ - ä¿®å¤ç‰ˆæœ¬"""

        # ğŸ”§ æ–°å¢ï¼šæ£€æŸ¥æ˜¯å¦æ˜¯æ–°éœ€æ±‚
        is_new_requirement = context.get('is_new_requirement', False) if context else False

        # ä½¿ç”¨æ›´æ¸…æ™°çš„æç¤ºè¯ï¼Œé¿å…æ ¼å¼åŒ–é—®é¢˜
        prompt = """ä½ æ˜¯ä¸€ä¸ªè™šæ‹Ÿæ˜Ÿåº§å‚æ•°è¯†åˆ«ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ†æç”¨æˆ·éœ€æ±‚ï¼Œè¯†åˆ«å‡ºç”¨æˆ·æ˜ç¡®æåˆ°çš„å‚æ•°ï¼Œä¸è¦è‡ªè¡Œæ¨æ–­æˆ–è¡¥å……ã€‚

    ## é‡è¦åŸåˆ™ï¼š
    1. åªæå–ç”¨æˆ·æ˜ç¡®æåˆ°çš„å‚æ•°
    2. ä¸è¦è‡ªè¡Œæ¨æ–­æˆ–è¡¥å……å‚æ•°å€¼
    3. å¦‚æœç”¨æˆ·æ²¡æœ‰æ˜ç¡®è¯´æ˜æŸä¸ªå‚æ•°ï¼Œå°±ä¸è¦æå–è¯¥å‚æ•°
    4. **æ—¶é—´å‚æ•°å¿…é¡»ä¿ç•™å®Œæ•´çš„è¡¨è¾¾ï¼ŒåŒ…æ‹¬æ•°å­—å’Œå•ä½ï¼Œä¾‹å¦‚ï¼š"1ä¸ªæœˆ"ã€"æ¯å¤©1æ¬¡"ã€"æ¯å‘¨2æ¬¡"**
    5. **ä¸è¦åªæå–æ•°å­—ï¼Œå¿…é¡»åŒ…å«å•ä½å’Œå®Œæ•´æè¿°**
    """

        if is_new_requirement:
            prompt += """
    4. **æ³¨æ„ï¼šè¿™æ˜¯ä¸€ä¸ªæ–°çš„ç›‘æµ‹éœ€æ±‚ï¼Œè¯·å¿½ç•¥ä¹‹å‰çš„å¯¹è¯å†å²ï¼Œåªå…³æ³¨å½“å‰ç”¨æˆ·è¾“å…¥**
    5. ä¸è¦æ··å…¥ä»»ä½•å†å²å¯¹è¯ä¸­çš„å‚æ•°
    """

        prompt += """
    ## å‚æ•°ç±»åˆ«è¯´æ˜ï¼š
    1. **ç›‘æµ‹ç›®æ ‡ (monitoring_target)**ï¼š
       - åªæœ‰å½“ç”¨æˆ·æ˜ç¡®è¯´"ç›‘æµ‹XX"ã€"è§‚æµ‹XX"ã€"å…³æ³¨XX"æ—¶æ‰æå–
       - ä¾‹å¦‚ï¼š"ç›‘æµ‹æ°´è´¨"â†’æå–"æ°´è´¨å˜åŒ–"

    2. **è§‚æµ‹åŒºåŸŸ (observation_area)**ï¼š
       - åªæœ‰å½“ç”¨æˆ·æåˆ°å…·ä½“åœ°åæ—¶æ‰æå–
       - ä¾‹å¦‚ï¼š"é’æµ·æ¹–"ã€"åŒ—äº¬å¸‚"ã€"é•¿æ±ŸæµåŸŸ"

    3. **è§‚æµ‹é¢‘ç‡ (observation_frequency)**ï¼š
       - åªæœ‰å½“ç”¨æˆ·æ˜ç¡®è¯´æ˜é¢‘ç‡æ—¶æ‰æå–
       - **å¿…é¡»ä¿ç•™å®Œæ•´è¡¨è¾¾**ï¼Œä¾‹å¦‚ï¼š
         - "æ¯å¤©ä¸€æ¬¡" â†’ æå– "æ¯å¤©1æ¬¡"
         - "æ¯å‘¨ä¸¤æ¬¡" â†’ æå– "æ¯å‘¨2æ¬¡"
         - "æ¯æœˆä¸€æ¬¡" â†’ æå– "æ¯æœˆ1æ¬¡"
       - **é”™è¯¯ç¤ºä¾‹**ï¼šä¸è¦åªæå– "1"ï¼Œå¿…é¡»åŒ…å«"æ¯å¤©"ã€"æ¯å‘¨"ç­‰å•ä½
    
    4. **ç›‘æµ‹å‘¨æœŸ (monitoring_period)**ï¼š
       - åªæœ‰å½“ç”¨æˆ·æ˜ç¡®è¯´æ˜æ—¶é•¿æ—¶æ‰æå–
       - **å¿…é¡»ä¿ç•™å®Œæ•´è¡¨è¾¾**ï¼Œä¾‹å¦‚ï¼š
         - "ç›‘æµ‹3ä¸ªæœˆ" â†’ æå– "3ä¸ªæœˆ"
         - "ä¸€å¹´" â†’ æå– "1å¹´"
         - "é•¿æœŸç›‘æµ‹" â†’ æå– "é•¿æœŸç›‘æµ‹"
       - **é”™è¯¯ç¤ºä¾‹**ï¼šä¸è¦åªæå– "3" æˆ– "1"ï¼Œå¿…é¡»åŒ…å«æ—¶é—´å•ä½

    5. **ç©ºé—´åˆ†è¾¨ç‡ (spatial_resolution)**ï¼š
       - åªæœ‰å½“ç”¨æˆ·æ˜ç¡®è¦æ±‚åˆ†è¾¨ç‡æ—¶æ‰æå–
       - ä¾‹å¦‚ï¼š"é«˜åˆ†è¾¨ç‡"ã€"10ç±³åˆ†è¾¨ç‡"

    ## è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š
    è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ–‡å­—ï¼š
    {
      "extracted_parameters": {
        "å‚æ•°å": "å…·ä½“å‚æ•°"
      },
      "confidence": 0.9
    }

    ## ç¤ºä¾‹ï¼š
    ç”¨æˆ·è¾“å…¥ï¼š"æˆ‘éœ€è¦ç›‘æµ‹æŸ¬åŸ”å¯¨çš„å†œä¸šä¿¡æ¯"
    è¾“å‡ºï¼š
    {
      "extracted_parameters": {
        "monitoring_target": "å†œä¸šç›‘æµ‹",
        "observation_area": "æŸ¬åŸ”å¯¨"
      },
      "confidence": 0.9
    }

    ## å½“å‰ç”¨æˆ·éœ€æ±‚ï¼š
    """ + user_input

        # å¦‚æœæœ‰å·²çŸ¥å‚æ•°ä¸”ä¸æ˜¯æ–°éœ€æ±‚ï¼Œæ·»åŠ åˆ°æç¤ºä¸­
        if context and context.get('existing_params') and not is_new_requirement:
            prompt += "\n\n## å·²ç»è¯†åˆ«çš„å‚æ•°ï¼ˆè¯·å‹¿é‡å¤æå–ï¼‰ï¼š\n"
            for key, value in context['existing_params'].items():
                prompt += f"- {key}: {value}\n"

        prompt += "\n\nè¯·åˆ†æä¸Šè¿°ç”¨æˆ·éœ€æ±‚ï¼Œåªæå–æ˜ç¡®æåˆ°çš„å‚æ•°ï¼Œç›´æ¥è¾“å‡ºJSONæ ¼å¼ï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—ã€‚"

        return prompt

    def _select_relevant_examples(self, user_input: str, num_examples: int = 3) -> List[Dict]:
        """é€‰æ‹©ç›¸å…³çš„ç¤ºä¾‹æ¡ˆä¾‹"""
        # ç®€å•çš„å…³é”®è¯åŒ¹é…ï¼Œåç»­å¯ä»¥æ”¹è¿›ä¸ºè¯­ä¹‰ç›¸ä¼¼åº¦
        user_input_lower = user_input.lower()

        scored_examples = []
        for example in self.example_cases:
            score = 0
            # æ£€æŸ¥å…³é”®è¯åŒ¹é…
            for keyword in example.get('keywords', []):
                if keyword in user_input_lower:
                    score += 1

            # æ£€æŸ¥ç›‘æµ‹ç›®æ ‡åŒ¹é…
            if 'parameters' in example:
                target = example['parameters'].get('monitoring_target', '')
                if target and target in user_input:
                    score += 2

            scored_examples.append((score, example))

        # æŒ‰åˆ†æ•°æ’åºå¹¶è¿”å›å‰Nä¸ª
        scored_examples.sort(key=lambda x: x[0], reverse=True)
        return [ex[1] for ex in scored_examples[:num_examples]]

    def _parse_parameter_extraction(self, model_output: str) -> Dict[str, Any]:
        """è§£ææ¨¡å‹è¾“å‡ºçš„å‚æ•° - å¢å¼ºç‰ˆæœ¬ï¼ˆåŒ…å«å‚æ•°åç§°æ˜ å°„ï¼‰"""

        # å®šä¹‰å‚æ•°åç§°æ˜ å°„è¡¨
        PARAMETER_NAME_MAPPING = {
            # æ—¶é—´ç›¸å…³å‚æ•°æ˜ å°„
            "monitoring_frequency": "observation_frequency",
            "monitor_frequency": "observation_frequency",
            "observing_frequency": "observation_frequency",
            "ç›‘æµ‹é¢‘ç‡": "observation_frequency",

            # å‘¨æœŸç›¸å…³å‚æ•°æ˜ å°„
            "monitor_period": "monitoring_period",
            "observation_period": "monitoring_period",
            "monitoring_duration": "monitoring_period",
            "ç›‘æµ‹å‘¨æœŸ": "monitoring_period",

            # ç›®æ ‡ç›¸å…³å‚æ•°æ˜ å°„
            "monitor_target": "monitoring_target",
            "observation_target": "monitoring_target",
            "monitoring_objective": "monitoring_target",
            "ç›‘æµ‹ç›®æ ‡": "monitoring_target",

            # åŒºåŸŸç›¸å…³å‚æ•°æ˜ å°„
            "monitor_area": "observation_area",
            "monitoring_area": "observation_area",
            "observation_region": "observation_area",
            "è§‚æµ‹åŒºåŸŸ": "observation_area",

            # èŒƒå›´ç›¸å…³å‚æ•°æ˜ å°„
            "cover_range": "coverage_range",
            "monitoring_range": "coverage_range",
            "observation_range": "coverage_range",
            "è¦†ç›–èŒƒå›´": "coverage_range"
        }

        try:
            # æ¸…ç†æ¨¡å‹è¾“å‡º
            cleaned_output = model_output.strip()
            logger.debug(f"æ¨¡å‹åŸå§‹è¾“å‡º: {cleaned_output[:500]}...")

            # å°è¯•å¤šç§æ–¹å¼æå–JSON
            json_str = None

            # æ–¹æ³•1ï¼šç›´æ¥å°è¯•è§£ææ•´ä¸ªè¾“å‡º
            try:
                result = json.loads(cleaned_output)
                if isinstance(result, dict) and 'extracted_parameters' in result:
                    extracted_params = result.get('extracted_parameters', {})

                    # åº”ç”¨å‚æ•°åç§°æ˜ å°„
                    mapped_params = {}
                    for key, value in extracted_params.items():
                        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ˜ å°„
                        mapped_key = PARAMETER_NAME_MAPPING.get(key, key)
                        mapped_params[mapped_key] = value

                        if mapped_key != key:
                            logger.info(f"å‚æ•°åç§°æ˜ å°„: {key} -> {mapped_key}")

                    return mapped_params
            except:
                pass

            # æ–¹æ³•2ï¼šæŸ¥æ‰¾JSONå—
            import re
            json_pattern = r'\{[^{}]*\{[^{}]*\}[^{}]*\}'
            json_matches = re.findall(json_pattern, cleaned_output, re.DOTALL)

            for match in json_matches:
                try:
                    result = json.loads(match)
                    if isinstance(result, dict) and 'extracted_parameters' in result:
                        extracted_params = result.get('extracted_parameters', {})

                        # åº”ç”¨å‚æ•°åç§°æ˜ å°„
                        mapped_params = {}
                        for key, value in extracted_params.items():
                            mapped_key = PARAMETER_NAME_MAPPING.get(key, key)
                            mapped_params[mapped_key] = value

                            if mapped_key != key:
                                logger.info(f"å‚æ•°åç§°æ˜ å°„: {key} -> {mapped_key}")

                        return mapped_params
                except:
                    continue

            # æ–¹æ³•3ï¼šæ›´å®½æ¾çš„JSONæå–
            brace_pattern = r'\{([^}]+)\}'
            brace_matches = re.findall(brace_pattern, cleaned_output, re.DOTALL)

            for content in brace_matches:
                try:
                    test_json = '{' + content + '}'
                    result = json.loads(test_json)
                    if isinstance(result, dict):
                        # åº”ç”¨å‚æ•°åç§°æ˜ å°„
                        mapped_params = {}
                        for key, value in result.items():
                            mapped_key = PARAMETER_NAME_MAPPING.get(key, key)
                            mapped_params[mapped_key] = value

                            if mapped_key != key:
                                logger.info(f"å‚æ•°åç§°æ˜ å°„: {key} -> {mapped_key}")

                        return mapped_params
                except:
                    pass

            # æ–¹æ³•4ï¼šæ‰‹åŠ¨æå–å…³é”®ä¿¡æ¯
            logger.warning("æ— æ³•è§£æJSONï¼Œå°è¯•æ‰‹åŠ¨æå–å‚æ•°")
            params = {}

            # å®šä¹‰æ›´å…¨é¢çš„å‚æ•°æå–æ¨¡å¼
            param_patterns = {
                # observation_frequency çš„å„ç§å¯èƒ½å½¢å¼
                "observation_frequency": [
                    r'"observation_frequency"\s*:\s*"([^"]+)"',
                    r'"monitoring_frequency"\s*:\s*"([^"]+)"',
                    r'"monitor_frequency"\s*:\s*"([^"]+)"',
                    r'"observing_frequency"\s*:\s*"([^"]+)"'
                ],

                # monitoring_period çš„å„ç§å¯èƒ½å½¢å¼
                "monitoring_period": [
                    r'"monitoring_period"\s*:\s*"([^"]+)"',
                    r'"monitor_period"\s*:\s*"([^"]+)"',
                    r'"observation_period"\s*:\s*"([^"]+)"',
                    r'"monitoring_duration"\s*:\s*"([^"]+)"'
                ],

                # monitoring_target çš„å„ç§å¯èƒ½å½¢å¼
                "monitoring_target": [
                    r'"monitoring_target"\s*:\s*"([^"]+)"',
                    r'"monitor_target"\s*:\s*"([^"]+)"',
                    r'"observation_target"\s*:\s*"([^"]+)"',
                    r'"monitoring_objective"\s*:\s*"([^"]+)"'
                ],

                # observation_area çš„å„ç§å¯èƒ½å½¢å¼
                "observation_area": [
                    r'"observation_area"\s*:\s*"([^"]+)"',
                    r'"monitor_area"\s*:\s*"([^"]+)"',
                    r'"monitoring_area"\s*:\s*"([^"]+)"',
                    r'"observation_region"\s*:\s*"([^"]+)"'
                ]
            }

            # å°è¯•æ‰€æœ‰æ¨¡å¼
            for param_key, patterns in param_patterns.items():
                for pattern in patterns:
                    match = re.search(pattern, cleaned_output)
                    if match:
                        params[param_key] = match.group(1)
                        logger.info(f"æ‰‹åŠ¨æå–åˆ°å‚æ•° {param_key}: {match.group(1)}")
                        break

            if params:
                logger.info(f"æ‰‹åŠ¨æå–åˆ°å‚æ•°: {params}")
                return params

        except Exception as e:
            logger.error(f"è§£ææ¨¡å‹è¾“å‡ºå¤±è´¥: {e}")
            logger.error(f"æ¨¡å‹è¾“å‡ºå†…å®¹: {model_output[:200]}...")
            import traceback
            traceback.print_exc()

        # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œä½¿ç”¨åŸºäºè§„åˆ™çš„å¤‡ç”¨æå–
        return self._fallback_extraction(model_output)

    def _fallback_extraction(self, text: str) -> Dict[str, Any]:
        """å¤‡ç”¨çš„è§„åˆ™æå–æ–¹æ³• - å¢å¼ºç‰ˆæœ¬"""
        params = {}

        # ç›‘æµ‹ç›®æ ‡æå–
        if any(keyword in text for keyword in ['ç›‘æµ‹æ°´è´¨', 'æ°´è´¨å˜åŒ–', 'æ°´è´¨ç›‘æµ‹']):
            params['monitoring_target'] = 'æ°´è´¨å˜åŒ–'
        elif any(keyword in text for keyword in ['ç›‘æµ‹å†œä¸š', 'å†œä¸šç›‘æµ‹', 'ä½œç‰©ç›‘æµ‹']):
            params['monitoring_target'] = 'å†œä¸šç›‘æµ‹'
        elif any(keyword in text for keyword in ['åŸå¸‚ç›‘æµ‹', 'åŸå¸‚æ‰©å¼ ', 'å»ºç­‘å˜åŒ–']):
            params['monitoring_target'] = 'åŸå¸‚æ‰©å¼ '

        # åœ°ç†ä½ç½®æå–
        import re

        # ä¸­å›½åœ°å
        chinese_locations = ['é’æµ·æ¹–', 'é•¿æ±Ÿ', 'é»„æ²³', 'å¤ªæ¹–', 'æ´åº­æ¹–', 'é„±é˜³æ¹–', 'ç æ±Ÿ', 'åŒ—äº¬', 'ä¸Šæµ·', 'æ­¦æ±‰']
        for loc in chinese_locations:
            if loc in text:
                params['observation_area'] = loc
                break

        # å›½å®¶åç§°
        countries = ['æŸ¬åŸ”å¯¨', 'è¶Šå—', 'æ³°å›½', 'è€æŒ', 'ç¼…ç”¸', 'é©¬æ¥è¥¿äºš', 'æ–°åŠ å¡', 'å°åº¦å°¼è¥¿äºš', 'è²å¾‹å®¾']
        for country in countries:
            if country in text:
                params['observation_area'] = country
                break

        # é¢‘ç‡æå–
        freq_patterns = {
            'æ¯å¤©': 'æ¯å¤©1æ¬¡',
            'æ¯æ—¥': 'æ¯å¤©1æ¬¡',
            'æ¯å‘¨': 'æ¯å‘¨2æ¬¡',
            'æ¯æœˆ': 'æ¯æœˆ1æ¬¡'
        }

        for pattern, value in freq_patterns.items():
            if pattern in text:
                params['observation_frequency'] = value
                break

        # åˆ†è¾¨ç‡æå–
        if any(keyword in text for keyword in ['é«˜åˆ†è¾¨ç‡', 'é«˜æ¸…', 'ç²¾ç»†']):
            params['spatial_resolution'] = 'high'
        elif any(keyword in text for keyword in ['ä¸­åˆ†è¾¨ç‡', 'ä¸­ç­‰åˆ†è¾¨ç‡']):
            params['spatial_resolution'] = 'medium'
        elif any(keyword in text for keyword in ['ä½åˆ†è¾¨ç‡', 'ç²—åˆ†è¾¨ç‡']):
            params['spatial_resolution'] = 'low'

        logger.info(f"å¤‡ç”¨æ–¹æ³•æå–çš„å‚æ•°: {params}")
        return params

    async def generate_clarification_questions(
            self,
            missing_params: List[str],
            context: Dict[str, Any] = None
    ) -> List[Dict[str, Any]]:
        """ä½¿ç”¨ä¹å·æ¨¡å‹ç”Ÿæˆæ™ºèƒ½æ¾„æ¸…é—®é¢˜"""
        try:
            prompt = self._build_question_generation_prompt(missing_params, context)
            response = await self.generate(prompt, max_tokens=1000)

            questions = self._parse_generated_questions(response, missing_params)
            return questions
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ¾„æ¸…é—®é¢˜å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            # è¿”å›é»˜è®¤é—®é¢˜
            return [self._get_default_question(param) for param in missing_params]

    def _build_question_generation_prompt(self, missing_params: List[str], context: Dict[str, Any]) -> str:
        """æ„å»ºé—®é¢˜ç”Ÿæˆçš„æç¤ºè¯"""

        # å‚æ•°æè¿°æ˜ å°„
        param_descriptions = {
            'observation_area': 'è§‚æµ‹åŒºåŸŸ - éœ€è¦ç›‘æµ‹çš„åœ°ç†ä½ç½®',
            'monitoring_target': 'ç›‘æµ‹ç›®æ ‡ - å…·ä½“è¦ç›‘æµ‹ä»€ä¹ˆå†…å®¹',
            'observation_frequency': 'è§‚æµ‹é¢‘ç‡ - å¤šä¹…è·å–ä¸€æ¬¡æ•°æ®',
            'monitoring_period': 'ç›‘æµ‹å‘¨æœŸ - æ€»ä½“ç›‘æµ‹æ—¶é•¿',
            'spatial_resolution': 'ç©ºé—´åˆ†è¾¨ç‡ - å½±åƒçš„æ¸…æ™°åº¦',
            'spectral_bands': 'å…‰è°±æ³¢æ®µ - éœ€è¦çš„æ•°æ®ç±»å‹',
            'analysis_requirements': 'åˆ†æéœ€æ±‚ - éœ€è¦è¿›è¡Œçš„åˆ†æç±»å‹'
        }

        prompt = """ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„è™šæ‹Ÿæ˜Ÿåº§åŠ©æ‰‹ã€‚ç”¨æˆ·æƒ³è¦è®¾è®¡è™šæ‹Ÿæ˜Ÿåº§æ–¹æ¡ˆï¼Œä½†ç¼ºå°‘ä¸€äº›å¿…è¦çš„å‚æ•°ä¿¡æ¯ã€‚
è¯·ä¸ºæ¯ä¸ªç¼ºå¤±çš„å‚æ•°ç”Ÿæˆä¸€ä¸ªè‡ªç„¶ã€å‹å¥½çš„æ¾„æ¸…é—®é¢˜ã€‚

## å·²çŸ¥ä¿¡æ¯ï¼š
"""

        if context and context.get('existing_params'):
            for key, value in context['existing_params'].items():
                desc = param_descriptions.get(key, key)
                prompt += f"- {desc}: {value}\n"
        else:
            prompt += "- æš‚æ— å·²çŸ¥å‚æ•°\n"

        prompt += "\n## ç¼ºå¤±çš„å‚æ•°ï¼š\n"
        for param in missing_params:
            desc = param_descriptions.get(param, param)
            prompt += f"- {param}: {desc}\n"

        prompt += """
## é—®é¢˜ç”Ÿæˆè¦æ±‚ï¼š
1. æ¯ä¸ªé—®é¢˜éƒ½è¦è‡ªç„¶ã€å‹å¥½ã€æ˜“æ‡‚
2. é¿å…ä½¿ç”¨è¿‡äºä¸“ä¸šçš„æœ¯è¯­ï¼Œæˆ–è€…è¦è§£é‡Šæ¸…æ¥š
3. å¯ä»¥æä¾›ä¸€äº›ä¾‹å­å¸®åŠ©ç”¨æˆ·ç†è§£
4. æ ¹æ®å·²çŸ¥ä¿¡æ¯è°ƒæ•´é—®é¢˜çš„è¡¨è¿°
5. è¾“å‡ºJSONæ ¼å¼ï¼ŒåŒ…å«questionsæ•°ç»„

## è¾“å‡ºæ ¼å¼ç¤ºä¾‹ï¼š
{
  "questions": [
    {
      "parameter": "observation_area",
      "question": "æ‚¨éœ€è¦ç›‘æµ‹å“ªä¸ªåœ°åŒºå‘¢ï¼Ÿå¯ä»¥æ˜¯å…·ä½“çš„æ¹–æ³Šã€åŸå¸‚æˆ–è€…åŒºåŸŸï¼Œæ¯”å¦‚é’æµ·æ¹–ã€åŒ—äº¬å¸‚ç­‰",
      "examples": ["é’æµ·æ¹–", "é•¿æ±ŸæµåŸŸ", "åŒ—äº¬å¸‚äº”ç¯å†…"],
      "hint": "ğŸ’¡ å¯ä»¥æä¾›åœ°åã€è¡Œæ”¿åŒºåŸŸæˆ–ç»çº¬åº¦èŒƒå›´"
    }
  ]
}

è¯·ç”Ÿæˆå‹å¥½çš„æ¾„æ¸…é—®é¢˜ï¼š
"""

        return prompt

    def _parse_generated_questions(self, model_output: str, missing_params: List[str]) -> List[Dict[str, Any]]:
        """è§£æç”Ÿæˆçš„é—®é¢˜"""
        questions = []

        try:
            # å°è¯•è§£æJSON
            import re
            json_match = re.search(r'\{[\s\S]*\}', model_output)
            if json_match:
                result = json.loads(json_match.group())
                generated_questions = result.get('questions', [])

                # ç¡®ä¿æ¯ä¸ªç¼ºå¤±å‚æ•°éƒ½æœ‰é—®é¢˜
                for param in missing_params:
                    question_data = next(
                        (q for q in generated_questions if q.get('parameter') == param),
                        None
                    )

                    if question_data:
                        questions.append({
                            'parameter_key': param,
                            'question': question_data.get('question', f"è¯·æä¾›{param}"),
                            'examples': question_data.get('examples', []),
                            'hint': question_data.get('hint', ''),
                            'type': 'text'  # é»˜è®¤ç±»å‹
                        })
                    else:
                        # ä½¿ç”¨é»˜è®¤é—®é¢˜
                        questions.append(self._get_default_question(param))

        except Exception as e:
            logger.error(f"è§£æç”Ÿæˆçš„é—®é¢˜å¤±è´¥: {e}")
            # ä½¿ç”¨é»˜è®¤é—®é¢˜
            for param in missing_params:
                questions.append(self._get_default_question(param))

        return questions

    def _get_default_question(self, param: str) -> Dict[str, Any]:
        """è·å–é»˜è®¤é—®é¢˜"""
        default_questions = {
            'observation_area': {
                'parameter_key': 'observation_area',
                'question': 'æ‚¨éœ€è¦ç›‘æµ‹å“ªä¸ªåœ°ç†åŒºåŸŸï¼Ÿ',
                'examples': ['é’æµ·æ¹–', 'é•¿æ±ŸæµåŸŸ', 'åŒ—äº¬å¸‚'],
                'hint': 'ğŸ’¡ å¯ä»¥æ˜¯å…·ä½“åœ°åã€è¡Œæ”¿åŒºåŸŸæˆ–ç»çº¬åº¦èŒƒå›´',
                'type': 'text'
            },
            'monitoring_target': {
                'parameter_key': 'monitoring_target',
                'question': 'æ‚¨çš„ä¸»è¦ç›‘æµ‹ç›®æ ‡æ˜¯ä»€ä¹ˆï¼Ÿ',
                'examples': ['æ°´è´¨å˜åŒ–', 'æ¤è¢«è¦†ç›–', 'åŸå¸‚æ‰©å¼ '],
                'hint': 'ğŸ’¡ è¯·å°½å¯èƒ½å…·ä½“æè¿°æ‚¨æƒ³è§‚æµ‹çš„å†…å®¹',
                'type': 'text'
            },
            'observation_frequency': {
                'parameter_key': 'observation_frequency',
                'question': 'æ‚¨éœ€è¦å¤šé•¿æ—¶é—´è·å–ä¸€æ¬¡è§‚æµ‹æ•°æ®ï¼Ÿ',
                'examples': ['æ¯å¤©1æ¬¡', 'æ¯å‘¨2æ¬¡', 'æ¯æœˆ1æ¬¡'],
                'hint': 'ğŸ’¡ é¢‘ç‡è¶Šé«˜ï¼Œæ—¶é—´åˆ†è¾¨ç‡è¶Šå¥½',
                'type': 'text'
            },
            'monitoring_period': {
                'parameter_key': 'monitoring_period',
                'question': 'æ‚¨è®¡åˆ’ç›‘æµ‹å¤šé•¿æ—¶é—´ï¼Ÿ',
                'examples': ['1ä¸ªæœˆ', '3ä¸ªæœˆ', '1å¹´', 'é•¿æœŸç›‘æµ‹'],
                'hint': 'ğŸ’¡ æ˜¯çŸ­æœŸé¡¹ç›®è¿˜æ˜¯é•¿æœŸç›‘æµ‹',
                'type': 'text'
            },
            'spatial_resolution': {
                'parameter_key': 'spatial_resolution',
                'question': 'æ‚¨éœ€è¦ä»€ä¹ˆçº§åˆ«çš„ç©ºé—´åˆ†è¾¨ç‡ï¼Ÿ',
                'examples': ['é«˜åˆ†è¾¨ç‡(<5ç±³)', 'ä¸­åˆ†è¾¨ç‡(5-30ç±³)', 'ä½åˆ†è¾¨ç‡(>30ç±³)'],
                'hint': 'ğŸ’¡ åˆ†è¾¨ç‡è¶Šé«˜ï¼Œèƒ½çœ‹åˆ°çš„ç»†èŠ‚è¶Šå¤š',
                'type': 'text'
            }
        }

        return default_questions.get(param, {
            'parameter_key': param,
            'question': f'è¯·æä¾›{param}ä¿¡æ¯',
            'examples': [],
            'hint': '',
            'type': 'text'
        })

    async def analyze_user_response(self, response: str, pending_questions: List[Dict]) -> Dict[str, Any]:
        """ä½¿ç”¨ä¹å·æ¨¡å‹åˆ†æç”¨æˆ·å›å¤"""
        try:
            prompt = f"""åˆ†æç”¨æˆ·å¯¹å‚æ•°æ¾„æ¸…é—®é¢˜çš„å›å¤ï¼Œæå–å‚æ•°å€¼ã€‚

## å¾…å›ç­”çš„é—®é¢˜ï¼š
"""
            for q in pending_questions:
                prompt += f"- {q['parameter_key']}: {q['question']}\n"

            prompt += f"\n## ç”¨æˆ·å›å¤ï¼š\n{response}\n"
            prompt += """
## ä»»åŠ¡ï¼š
1. åˆ†æç”¨æˆ·å›å¤ä¸­åŒ…å«çš„å‚æ•°å€¼
2. å°†å›å¤å†…å®¹æ˜ å°„åˆ°å¯¹åº”çš„å‚æ•°
3. å¦‚æœç”¨æˆ·æƒ³è·³è¿‡æˆ–ä½¿ç”¨é»˜è®¤å€¼ï¼Œæ ‡è®°ä¸º"skip"

è¾“å‡ºJSONæ ¼å¼ï¼š
{
  "parsed_parameters": {
    "å‚æ•°å": "å‚æ•°å€¼"
  },
  "skip_remaining": false
}
"""

            model_response = await self.generate(prompt, max_tokens=400)
            return self._parse_user_response_analysis(model_response)
        except Exception as e:
            logger.error(f"åˆ†æç”¨æˆ·å›å¤å‡ºé”™: {e}")
            return {
                "parsed_parameters": {},
                "skip_remaining": False
            }

    def _parse_user_response_analysis(self, model_output: str) -> Dict[str, Any]:
        """è§£æç”¨æˆ·å›å¤åˆ†æç»“æœ"""
        try:
            import re
            json_match = re.search(r'\{[\s\S]*\}', model_output)
            if json_match:
                return json.loads(json_match.group())
        except Exception as e:
            logger.error(f"è§£æç”¨æˆ·å›å¤åˆ†æå¤±è´¥: {e}")

        return {
            "parsed_parameters": {},
            "skip_remaining": False
        }

    def close(self):
        """æ¸…ç†èµ„æº"""
        self.executor.shutdown(wait=True)
        if self.model:
            del self.model
        if self.tokenizer:
            del self.tokenizer
        torch.cuda.empty_cache()


# å•ä¾‹æ¨¡å¼
_jiuzhou_instance = None


def get_jiuzhou_manager() -> JiuzhouModelManager:
    """è·å–ä¹å·æ¨¡å‹ç®¡ç†å™¨å•ä¾‹"""
    global _jiuzhou_instance
    if _jiuzhou_instance is None:
        _jiuzhou_instance = JiuzhouModelManager()
    return _jiuzhou_instance