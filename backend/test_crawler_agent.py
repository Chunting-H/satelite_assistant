# backend/test_crawler_agent.py - çˆ¬è™«æ™ºèƒ½ä½“é›†æˆæµ‹è¯•

import os
import sys
import json
import asyncio
import logging
from pathlib import Path

# ç¡®ä¿é¡¹ç›®æ ¹ç›®å½•åœ¨sys.pathä¸­
current_file = Path(__file__).resolve()
project_root = current_file.parent
sys.path.append(str(project_root))

from backend.src.tools.crawler_agent.crawler_workflow import crawler_workflow
from backend.src.tools.sate_search.satellite_crawler import SatelliteCrawler
from backend.src.tools.sate_search.satellite_data_processor import SatelliteDataProcessor

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s"
)
logger = logging.getLogger(__name__)


async def test_crawler_workflow():
    """æµ‹è¯•å®Œæ•´çš„çˆ¬è™«å·¥ä½œæµ"""
    print("=" * 60)
    print("ğŸ§ª æµ‹è¯•çˆ¬è™«æ™ºèƒ½ä½“å·¥ä½œæµ")
    print("=" * 60)
    
    try:
        # 1. åˆ›å»ºçˆ¬å–ä»»åŠ¡
        print("\nğŸ“‹ åˆ›å»ºçˆ¬å–ä»»åŠ¡...")
        job_id = await crawler_workflow.create_crawl_job(
            target_sites=["Gunter's Space Page"],
            keywords=[],
            max_satellites=3  # é™åˆ¶ä¸º3ä¸ªï¼ŒåŠ å¿«æµ‹è¯•é€Ÿåº¦
        )
        print(f"âœ… ä»»åŠ¡åˆ›å»ºæˆåŠŸ: {job_id}")
        
        # 2. æ‰§è¡Œçˆ¬å–ä»»åŠ¡
        print("\nğŸš€ æ‰§è¡Œçˆ¬å–ä»»åŠ¡...")
        result = await crawler_workflow.execute_crawl_job(job_id)
        print(f"âœ… ä»»åŠ¡æ‰§è¡Œå®Œæˆ: {result}")
        
        # 3. è·å–çˆ¬å–æ—¥å¿—
        print("\nğŸ“ è·å–çˆ¬å–æ—¥å¿—...")
        logs = await crawler_workflow.get_crawl_logs(limit=5)
        print(f"âœ… è·å–åˆ° {len(logs)} æ¡æ—¥å¿—")
        if logs:
            latest_log = logs[0]
            print(f"æœ€æ–°æ—¥å¿—: æ–°å¢{latest_log.get('newDataCount', 0)}ä¸ªå«æ˜Ÿ")
        
        # 4. è·å–ç»Ÿè®¡ä¿¡æ¯
        print("\nğŸ“Š è·å–ç»Ÿè®¡ä¿¡æ¯...")
        stats = await crawler_workflow.get_crawl_statistics(days=7)
        print(f"âœ… ç»Ÿè®¡ä¿¡æ¯: æ€»çˆ¬å–{stats['total_crawls']}æ¬¡, æ–°å¢{stats['total_new_satellites']}ä¸ªå«æ˜Ÿ")
        
        print("\nğŸ‰ çˆ¬è™«å·¥ä½œæµæµ‹è¯•å®Œæˆ!")
        return True
        
    except Exception as e:
        print(f"\nâŒ çˆ¬è™«å·¥ä½œæµæµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


async def test_satellite_crawler():
    """æµ‹è¯•å«æ˜Ÿçˆ¬è™«"""
    print("\n=" * 60)
    print("ğŸ•·ï¸ æµ‹è¯•å«æ˜Ÿçˆ¬è™«")
    print("=" * 60)
    
    try:
        crawler = SatelliteCrawler()
        
        print("\nğŸ“¡ çˆ¬å–æœ€è¿‘å‘å°„çš„å«æ˜Ÿ...")
        satellites = await crawler.crawl_recent_satellites(max_satellites=2)
        
        print(f"âœ… æˆåŠŸçˆ¬å– {len(satellites)} ä¸ªå«æ˜Ÿ")
        for sat in satellites:
            print(f"  - {sat.get('satellite_name', 'Unknown')}: {sat.get('launch_date', 'Unknown')}")
        
        return len(satellites) > 0
        
    except Exception as e:
        print(f"\nâŒ å«æ˜Ÿçˆ¬è™«æµ‹è¯•å¤±è´¥: {str(e)}")
        return False


async def test_data_processor():
    """æµ‹è¯•æ•°æ®å¤„ç†å™¨"""
    print("\n=" * 60)
    print("ğŸ§¹ æµ‹è¯•æ•°æ®å¤„ç†å™¨")
    print("=" * 60)
    
    try:
        processor = SatelliteDataProcessor()
        
        # æ¨¡æ‹ŸåŸå§‹æ•°æ®
        raw_data = [
            {
                "satellite_name": "æµ‹è¯•å«æ˜Ÿ-001",
                "launch_date": "05.08.2025",
                "site": "Test Launch Site",
                "vehicle": "Test Rocket",
                "detailed_specs": {
                    "nation": "Test Country",
                    "type_application": "Earth observation",
                    "mass": "500 kg"
                },
                "source_url": "https://test.example.com"
            }
        ]
        
        print("\nğŸ”„ æ ¼å¼åŒ–æ•°æ®...")
        formatted_data = await processor.clean_and_format_data(raw_data)
        print(f"âœ… æˆåŠŸæ ¼å¼åŒ– {len(formatted_data)} ä¸ªå«æ˜Ÿ")
        
        if formatted_data:
            sat = formatted_data[0]
            print(f"  - å«æ˜Ÿåç§°: {sat.get('satelliteName', 'Unknown')}")
            print(f"  - æ‰€æœ‰è€…: {sat.get('owner', 'Unknown')}")
            print(f"  - åº”ç”¨ç±»å‹: {sat.get('applications', [])}")
        
        print("\nğŸ’¾ æ£€æŸ¥å¹¶å­˜å‚¨æ•°æ®...")
        storage_stats = await processor.check_and_store_satellites(formatted_data)
        print(f"âœ… å­˜å‚¨ç»Ÿè®¡: {storage_stats}")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ æ•°æ®å¤„ç†å™¨æµ‹è¯•å¤±è´¥: {str(e)}")
        return False


async def test_api_endpoints():
    """æµ‹è¯•APIç«¯ç‚¹ï¼ˆæ¨¡æ‹Ÿæµ‹è¯•ï¼‰"""
    print("\n=" * 60)
    print("ğŸŒ æµ‹è¯•APIç«¯ç‚¹")
    print("=" * 60)
    
    try:
        # æµ‹è¯•çˆ¬è™«å·¥ä½œæµç®¡ç†å™¨çš„æ ¸å¿ƒåŠŸèƒ½
        print("\nğŸ“‹ æµ‹è¯•ä»»åŠ¡ç®¡ç†...")
        
        # åˆ›å»ºä»»åŠ¡
        job_id = await crawler_workflow.create_crawl_job(
            target_sites=["Gunter's Space Page"],
            max_satellites=1
        )
        print(f"âœ… ä»»åŠ¡åˆ›å»º: {job_id}")
        
        # è·å–ä»»åŠ¡çŠ¶æ€
        status = crawler_workflow.get_job_status(job_id)
        print(f"âœ… ä»»åŠ¡çŠ¶æ€: {status['status'] if status else 'Not found'}")
        
        # åˆ—å‡ºä»»åŠ¡
        jobs = crawler_workflow.list_jobs()
        print(f"âœ… ä»»åŠ¡åˆ—è¡¨: {len(jobs)} ä¸ªä»»åŠ¡")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ APIç«¯ç‚¹æµ‹è¯•å¤±è´¥: {str(e)}")
        return False


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ¯ å¼€å§‹çˆ¬è™«æ™ºèƒ½ä½“é›†æˆæµ‹è¯•")
    print("â° æ³¨æ„: å®Œæ•´æµ‹è¯•å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´...")
    
    test_results = {}
    
    # è¿è¡Œæµ‹è¯•
    test_results['crawler'] = await test_satellite_crawler()
    test_results['processor'] = await test_data_processor()
    test_results['api'] = await test_api_endpoints()
    test_results['workflow'] = await test_crawler_workflow()
    
    # æ±‡æ€»ç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ“‹ æµ‹è¯•ç»“æœæ±‡æ€»")
    print("=" * 60)
    
    for test_name, result in test_results.items():
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{test_name.upper():<15} {status}")
    
    passed = sum(test_results.values())
    total = len(test_results)
    
    print(f"\næ€»è®¡: {passed}/{total} é¡¹æµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡! çˆ¬è™«æ™ºèƒ½ä½“å·²å‡†å¤‡å°±ç»ª!")
    else:
        print(f"\nâš ï¸ æœ‰ {total - passed} é¡¹æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³åŠŸèƒ½")
    
    return passed == total


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    success = asyncio.run(main())
    
    if success:
        print("\nğŸš€ å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼ä½¿ç”¨çˆ¬è™«æ™ºèƒ½ä½“:")
        print("1. å¯åŠ¨åç«¯æœåŠ¡: python backend/main.py")
        print("2. å¯åŠ¨å‰ç«¯æœåŠ¡: cd frontend && npm run dev")
        print("3. åœ¨å«æ˜Ÿç®¡ç†é¡µé¢ç‚¹å‡»'æ•°æ®æ›´æ–°'æŒ‰é’®")
        print("4. æˆ–ç›´æ¥è°ƒç”¨API: POST /api/crawl/start")
    else:
        print("\nâŒ æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯å¹¶ä¿®å¤ç›¸å…³é—®é¢˜")
        sys.exit(1)
